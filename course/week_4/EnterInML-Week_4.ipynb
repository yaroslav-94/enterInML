{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задач многомерной линейной регрессии с помощью сингулярного разложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многомерная линейна регрессия \n",
    "\n",
    "$f_1(x), ..., f_n(x)$ - числовые признаки   \n",
    "\n",
    "Модель многомерной линейной регрессии: $f(x, \\alpha)=\\sum_{1 \\leq j \\leq n}\\alpha_j f_j(x), \\alpha \\in \\mathbb{R}^n$\n",
    "\n",
    "Матричные обозначения: $F_{l*n}=\\begin{pmatrix}f_1(x_1) & ... & f_n(x_1) \\\\ ... & ... & ... \\\\ f_1(x_) & ... & f_n(x_l)  \\end{pmatrix}$ - матрица объекты-признаки, $y_{l*1}=\\begin{pmatrix}y_1 \\\\ ... \\\\ y_l \\end{pmatrix}$ - вектор ответов, $\\alpha_{n*1}=\\begin{pmatrix}\\alpha_1 \\\\ ... \\\\ \\alpha_n \\end{pmatrix}$ - вектор коэффициентов\n",
    "\n",
    "Функционал квадрата ошибки: $Q(\\alpha, X^l)=\\sum_{1 \\leq i \\leq l}(f(x_i, \\alpha)-y_i)^2=||F\\alpha-y||^2 \\to min_{\\alpha}$  \n",
    "\n",
    "\n",
    "### Нормальная система уравнений\n",
    "Необходимое условие минимума в матричном виде: $\\frac{\\partial Q}{\\partial \\alpha}=2F^T(F\\alpha-y)=0$, отсюда следует нормальная система задачаи МНК: $F^TF\\alpha=F^Ty$, $F^TF$ - матрица размерности n*n \n",
    "\n",
    "Решение системы: $\\alpha^*=(F^TF)^{-1}FTy=F^+y$. Значение функционала: $Q(\\alpha^*)=||P_Fy-y||^2$, $P_F=FF^+=F(F^TF)^{-1}F^T$ - проекционная матрица\n",
    "\n",
    "\n",
    "### Сингулярное разложение\n",
    "Произвольная l*n матрица представима в виде сингулярногоразложения: $F=VDU^T$, $V,\\ U^T$ - ортогональные, D - диагональна\n",
    "\n",
    "Основыне свойства:\n",
    "- l*n матрица $V=(v_1,...,v_n)$ - ортогональна, $V^TV=I_n$, $v_j$ - собственные векторы матрицы $FF^T$\n",
    "- n*n матрица $U=(u_1, ..., u_n)$ - ортогональна, $U^TU=I_n$, $u_j$ - собственные векторы матрицы $F^TF$\n",
    "- n*n матрица D - диагональна, $D=diag(\\sqrt{\\lambda_1}, ..., \\sqrt{\\lambda_n})$, $\\lambda_j\\geq0$ - собственные значения матриц $F^TF$ и $FF^T$, $\\sqrt{\\lambda_j}$ - сингулярные числа матрицы F\n",
    "\n",
    "\n",
    "### Решение МНК через сингулярное разложение\n",
    "Псевдообратная матрица $F^+$, вектор МНК-решения $\\alpha^*$, МНК-аппроксимация целевоого вектора $F\\alpha^*$:\n",
    "\n",
    "$F^+=(UDV^TVDU^T)^{-1}UDV^T=UD^{-1}V^T=\\sum_{1 \\leq j \\leq n}\\frac{1}{\\sqrt{\\lambda_j}}u_jv_j ^T$\n",
    "\n",
    "$\\alpha^*=F^+y=UD^{-1}V^Ty=\\sum_{1 \\leq j \\leq n}\\frac{1}{\\sqrt{\\lambda_j}}u_j(v_j ^Ty)$\n",
    "\n",
    "$F\\alpha^*=P_Fy=(VDU^T)UD^{-1}V^Ty=VV^Ty=\\sum_{1 \\leq j \\leq n}v_j(v_j ^Ty)$\n",
    "\n",
    "$||\\alpha^*||^2=||D^{-1}V^Ty||^2=\\sum_{1 \\leq j \\leq n}\\frac{1}{\\lambda_j}(v_j ^Ty)^2$\n",
    "\n",
    "Появляется проблема мультиколлинеарности: $\\lambda_j \\to 0$\n",
    "\n",
    "\n",
    "### Проблема мультиколлинеарности и переобучения\n",
    "Если имеются сингулярные числа, близкие к 0, то:\n",
    "- матрица $\\sum=F^TF$ плохо обусловлена\n",
    "- решение становится неустойчивым и неинтерпретируемым, слишком большие коэффициенты $||\\alpha^*_j||$ разных знаков\n",
    "- возникает переобучение: \n",
    "    - на обучении $Q(\\alpha^*, X^l)=||F\\alpha^*-y||^2$ мало, \n",
    "    - на контроле $Q(\\alpha^*, X^l)=||F'\\alpha^*-y'||^2$ велико\n",
    "\n",
    "Стратегии устранения мультиколлинеарности и переобучения: \n",
    "- отбор признаков: $f_1, ..., f_n \\to f_{j_1}, ..., f_{j_m}, m\\ll n$\n",
    "- регуляризация: $||\\alpha||\\to min$\n",
    "- преобразование признаков: $f_1, ..., f_n \\to g_1, ..., g_m, m\\ll n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гребневая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация (гребневая регрессия)\n",
    "Штраф за увелчение нормы вектора весов $||\\alpha||$: $Q_{\\tau}(\\alpha)=||F\\alpha-y||^2+\\frac{\\tau}{2}||\\alpha||^2$, $\\tau$ - неотрицательный параметр регуляризации\n",
    "\n",
    "Модифицированное МНК-решение ($\\tau I_n$ - гребнь): $\\alpha^* _{\\tau}=(F^TF+\\tau I_n)^{-1}F^Ty$\n",
    "\n",
    "Преимущество сингулярного разложения: можно подбирать параметр $\\tau$ вычислив SVD только один раз\n",
    "\n",
    "\n",
    "### Регуляризованный МНК через сингулярное разложение\n",
    "Вектор регуляризованного МНК-решения $\\alpha^* _{\\tau}$ МНК-фппроксимация целевого вектора $F\\alpha^* _{\\tau}$:    \n",
    "$\\alpha^* _{\\tau}=U(D^2+\\tau I_n)^{-1}DV^Ty=\\sum_{1 \\leq j \\leq n}\\frac{\\sqrt{\\lambda_j}}{\\lambda_j+\\tau}u_j(v_j^Ty)$\n",
    "\n",
    "$F\\alpha^* _{\\tau}=VDU^T\\alpha^* _{\\tau}=V diag(\\frac{\\sqrt{\\lambda_j}}{\\lambda_j+\\tau})V^Ty=\\sum_{1 \\leq j \\leq n}\\frac{\\sqrt{\\lambda_j}}{\\lambda_j+\\tau}v_j(v_j^Ty)$\n",
    "\n",
    "$||\\alpha^* _{\\tau}||^2=||(D^2+\\tau I_n)^{-1}DV^Ty||^2=\\sum_{1 \\leq j \\leq n}\\frac{\\lambda_j}{(\\lambda_j+\\tau)^2} (v_j^Ty)^2$\n",
    "\n",
    "$F\\alpha_{\\tau} ^*\\neq F\\alpha^*$, но из-за коэффициента становится устойчивым\n",
    "\n",
    "\n",
    "### Выбор параметра регуляризации $\\tau$\n",
    "Контрольная выборка: $X^k=(x_i',y_i')_{i=1} ^k$, $F'_{n*n}=\\begin{pmatrix} f_1(x_1') & ... & f_n(x_1') \\\\ ... & ... & ... \\\\ f_1(x_k') & ... & f_n(x_k')\\end{pmatrix}$ - новая выборка для поиска коэффициента, $y'_{k*1}=\\begin{pmatrix} y'_1 \\\\ ... \\\\ y'_k \\end{pmatrix}$ - соответствующий столбец ответов\n",
    "\n",
    "Вычисление функционала Q на контрольных данных T раз потребует $O(kn^2+knT)$ операций: $Q(\\tau)=||F'\\alpha^* _{\\tau}-y'||^2=||F' U diag(\\frac{\\sqrt{\\lambda_i}}{\\lambda_i+\\tau})V^Ty-y'||^2$\n",
    "\n",
    "Зависимость $Q(\\tau)$ обычно имееет характерный минимум\n",
    "\n",
    "\n",
    "### Регуляризация сокращает \"эффективную размерность\"\n",
    "Сжатие или сокращение весов: $||\\alpha_{\\tau} ^*||=\\sum_{1 \\leq j \\leq n} \\frac{\\lambda_j}{(\\lambda_j+\\tau)^2}(v_j ^Ty)^2 < ||\\alpha^*||^2=\\sum_{1 \\leq j \\leq n}\\frac{1}{\\lambda_j(v_j ^Ty)^2}$\n",
    "\n",
    "Роль размерности играет след проекционной матрицы: $trF(F^TF)^{-1}F^T=tr(F^TF)^{-1}F^TF=trI_n=n$\n",
    "\n",
    "При использовании регуляризации: $trF(F^TF+\\tau I_n)^{-1}F^T=tr (diag(\\frac{\\lambda_j}{(\\lambda_j+\\tau)^2}))=\\sum_{1 \\leq j \\leq n}\\frac{\\lambda_j}{(\\lambda_j+\\tau)^2}<n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO приводит к отбору признаков в линейных моделях\n",
    "Постановка задачи:\n",
    "- $Q(\\alpha)=||F\\alpha-y||^2 \\to min_{\\alpha}, \\sum_{1 \\leq j \\leq n}|\\alpha_j|<\\varkappa$\n",
    "- $Q(\\alpha)=||F\\alpha-y||^2+\\tau\\sum_{1 \\leq j \\leq n}|\\alpha_j| \\to min_{\\alpha}$\n",
    "\n",
    "После замены переменных \n",
    "$\\begin{cases}\\alpha_j=\\alpha_j ^+ - \\alpha_j ^- \\\\ |\\alpha_j|=\\alpha_j^++\\alpha_j^- \\end{cases}$, $\\alpha_j^+\\geq0, \\alpha_j^-\\geq0$\n",
    "\n",
    "ограничения принимают канонический вид: $\\sum_{1 \\leq j \\leq n}\\alpha_j^++\\alpha_j^-\\geq\\varkappa,\\ \\alpha_j^+\\geq0,\\ \\alpha_j^-\\geq0$\n",
    "\n",
    "Чем меньше $\\varkappa$, тем больше j таких, что  $\\alpha_j^+=\\alpha_j^-=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод главных компонент: постановка задачи\n",
    "$f_1(x),...,f_n(x)$ - исходные числовые признаки    \n",
    "$g_1(x),...,g_m(x)$ - новые числовые признаки $m \\leq n$\n",
    "\n",
    "Требование: старые признаки должны линейно восстанавливаться по новым: $\\hat{f}_j(x)=\\sum_{s=1}^mg_s(x)u_{js}, j=1,...,n\\ \\forall x \\in X$\n",
    "\n",
    "как можно точнее на обучающей выборке $x_1, ..., x_l$: $\\sum_{i=1}^l\\sum_{j=1}^n(\\hat{f}_j(x_i)-f_j(x_i))^2 \\to min_{\\{g_s(x_i)\\},\\{u_{js}\\}}$\n",
    "\n",
    "\n",
    "### Матричные обозначения\n",
    "Матрицы \"объекты-признаки\", старая и новая:   \n",
    "$F_{l*n}=\\begin{pmatrix} f_1(x_1) & ... & f_n(x_1)\\\\ ... & ... & ... \\\\ f_1(x_l) & ... & f_n(x_l) \\end{pmatrix}$\n",
    "$G_{l*m}=\\begin{pmatrix} g_1(x_1) & ... & g_m(x_1)\\\\ ... & ... & ... \\\\ g_1(x_l) & ... & g_m(x_l) \\end{pmatrix}$\n",
    "\n",
    "Матрица лнейного преобразования новых признаков в старые: $U_{n*m}=\\begin{pmatrix} u_{11} & ... & u_{1m}\\\\ ... & ... & ... \\\\ u_{n1} & ... & u_{nm} \\end{pmatrix},\\ \\hat{F}=GU^T\\simeq F$\n",
    "\n",
    "Новые признаки G и преобразование U: $\\sum_{i=1}^l\\sum_{j=1}^n(\\hat{f}_j(x_i)-f_j(x_i))^2=||GU^T-F||^2 \\to min_{G,U}$\n",
    "\n",
    "\n",
    "### Основная теорема метода главных компонент\n",
    "Если $m\\leq rang(F)$, то минимум $||GU^T-F||^2$ достигается, когда столбцы U - собственные вектора соответствующие собственым значениям $\\lambda_1, ..., \\lambda_m$, а матрица $G=FU$\n",
    "\n",
    "- Матрица U ортонормированна: $U^TU=I_n$\n",
    "- Матрица G ортогональна: $G^TG=\\Lambda=diag(\\lambda_1,...,\\lambda_m)$\n",
    "- $U\\Lambda=F^TFU;\\ G\\Lambda=FF^TG$\n",
    "- $||GU^T-F||^2=||F||^2-tr(\\Lambda)=\\sum_{j=m+1}^n\\lambda_j$\n",
    "\n",
    "\n",
    "### Связь с сингулярным разложением\n",
    "Если взять m=n, то:\n",
    "- $||GU^T-F||^2=0$\n",
    "- представление $\\hat{F}=GU^T=F$ точное и совпадает с сингулярным разложением при $G=V\\sqrt{\\Lambda}$: $F=GU^T=V\\sqrt{\\Lambda}U^T,\\ U^TU=I_m,\\ V^TV=I_m$\n",
    "- линейное преобразование U работает в обе стороны: $F=GU^T,\\ G=FU$\n",
    "\n",
    "Новые признаки некоррелированы, преобразование U называется декоррелирующим\n",
    "\n",
    "\n",
    "### Эффективная размерность выборки\n",
    "Упорядочим собственные значения $F^TF$ по убыванию: $\\lambda_1\\geq...\\geq\\lambda_n\\geq0$\n",
    "Эффективная размерность выборки - это наименьшее целое m, при котором $E_m=\\frac{||GU^T-F||^2}{||F||^2}=\\frac{\\lambda_{m+1}+...+\\lambda_n}{\\lambda_1+...+\\lambda_n}\\leq\\epsilon$\n",
    "Критерий крутого склона: находим m: $E_{m-1}\\gg E_m$\n",
    "\n",
    "\n",
    "### Решение задачи наименьщих квадратов для многомерной линейной регрессии\n",
    "Задача ниаменьших квадратов для МЛР: $||F\\alpha-y||^2 \\to min_{\\alpha}$    \n",
    "Заменим $F_{l*n}$ на ее приближение $G_{l*m}U_{m*n}^T$, предполагая $m\\leq n$: $||GU^T\\alpha-y||^2=||G\\beta-y||^2 \\to min_{beta}$   \n",
    "Связь нового и старого вектора коэффициентов: $\\beta=U^T\\alpha,\\ \\alpha=U\\beta$   \n",
    "    Решение задачи наименьших квадратов относительно $\\beta$ (единственное отличие m слагаемых вместо n): $\\beta^*=D^{-1}V^Ty,\\ \\alpha^*=UD^{-1}V^Ty=\\sum_{j=1} ^m\\frac{1}{\\sqrt{\\lambda_j}}u_j(v_j ^Ty),\\ G\\beta^*=VV^Ty=\\sum_{j=1}^m v_j(v_j ^Ty)$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
