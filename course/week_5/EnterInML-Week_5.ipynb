{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бэггинг и случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простое голосование классификаторов\n",
    "Обучающая выборка: $X^l=(x_i, y_i)_{i=1} ^l,\\ x_i \\in X,\\ y_i \\in \\{-1, +1\\}$    \n",
    "Базовые классификаторы: $b_1(x),...,b_T(x),\\ b_t:X \\to \\{-1, +1\\}$\n",
    "\n",
    "Простое голосование базовых классификаторов: $a(x)=\\sum_{t=1}^Tb_t(x)$\n",
    "Композиция a(x) ожет быть лучше базовых $b_1(x),...,b_T(x)$, если они лучше случайного гадания и достаточно различны\n",
    "\n",
    "Способы повышения различности базовыз классификаторов:\n",
    "- обучение по случайным подвыборкам\n",
    "- обучение по выборке со случайными весами объектов\n",
    "- обучение по случайным подмножествам признаков\n",
    "- использование различных моделей классификации\n",
    "- использование различных начальных приближений\n",
    "- использование рандомизации при обучении классификаторов\n",
    "\n",
    "\n",
    "### Бэггинг и метод случайных подпространств\n",
    "Бэггинг (bootstrap aggregating): $b_t(x)$ обучаются независимо по случайным подвыборкам длины l с повторениями, доля объектов, попадающих в выборку: $(1-\\frac{1}{e})\\supset0.632$\n",
    "\n",
    "Метод случайных подпространств: $b_t(x)$ обучаются независимо по случайным подмножествам n' признаков\n",
    "\n",
    "Совмещение обеих идей в одном алгоритме: $\\mathscr{J}=\\{f_1,...,f_n\\}$ - признаки, $\\mu(\\mathscr{F}, U)$ - метод обучения алгоритма по подвыборке $U\\subseteq X^l$, использующий только признаки из $\\mathscr{F}\\subseteq\\mathscr{J}$\n",
    "\n",
    "\n",
    "### Бэггинг и метод случайных подпространств\n",
    "Вход: обучающая выборка $X^l$, параметры: T, l'-длина обучающий подвыборок, n'-длина признакового подописания, $\\epsilon_1$-порог качества базовых алгоритмов на обучении, $\\epsilon_2$-порог качества базовых алгоритмов на контроле\n",
    "Выход: базовые алгоритмы $b_t,\\ t=1,...,T$\n",
    "\n",
    "1) для всех $t=1,...,T$:   \n",
    "2) $U_t:=случайное\\ подмножество\\ X^l\\ длины\\ l'$    \n",
    "3) $\\mathscr{F}:=случайное\\ подмножество\\ \\mathscr{J}\\ длины\\ n'$      \n",
    "4) $b_t:=\\mu(\\mathscr{F}_t, U_t)$      \n",
    "5) if $Q(b_t, U_t)>\\epsilon_1$ или $Q(b_t, X^l|U_t)>\\epsilon_2$     \n",
    "6) не включать $b_t$ в композицию   \n",
    "\n",
    "Композиция - простое голосование: $a(x)=sign\\sum_{t=1}^Tb_t(x)$\n",
    "\n",
    "\n",
    "### Случайный лес\n",
    "Обучение случайного леса:\n",
    "- бэггинг над решающими деревьями\n",
    "- усечение дерева не производится\n",
    "- признак в каждой вершине дерева выбирается\n",
    "- признак в каждой вершине дерева выбирается из случайного подмножества k из n признаков\n",
    "- для регрессии рекмендуется k=[n\\3]\n",
    "- для классификации рекомендуется $k=[\\sqrt{n}]$\n",
    "\n",
    "Подбор числа деревьев Т по критерию out-of-bag - число ошибок на объектах $x_i$, если не учитывать голоса деревьев, для которых $x_i$ был обучающим: $out-of-bag(a)=\\sum_{i=1}^l[sign(\\sum_{t=1}^T[x_i\\notin U_t]b_t(x_i))\\neq y_i]\\to min$ - несмещенная оценка обобщающей способности\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейные композиции для классификации и регрессии\n",
    "Задача восстановления зависимости y: $X \\to Y$ по точкам обучающей выборки $(x_i, y_i),\\ y_i=y(x_i),\\ i=1,...,l$\n",
    "\n",
    "Определение: линейной композицией базовых алгоритмов $a_t(x)=C(b_t(x)),\\ t=1,...,T$ называется суперпозиция функций $a(x)=C\\sum_{t=1}^Ta_t b_t(x),\\ C:\\mathbb{R}\\to Y$ - решающее правило, $a_t\\geq0$\n",
    "\n",
    "Для классификации на 2 класса: C(b)=sign(b), a(x)=sign(b(x))\n",
    "Для регрессии: C(b)=b, a(x)=b(x)\n",
    "\n",
    "\n",
    "### Градиентный бустинг\n",
    "Линейная композиция базовых алгоритмов: $b(x)=\\sum_{t=1}^Ta_tb_t(x),\\ x\\in X,\\ a_t \\in \\mathbb{R}_+$\n",
    "Функционал качества с произвольной функцией потерь $\\mathcal{L}(b,y)$: $Q(a, b)=\\sum_{i=1}^l\\mathcal{L}(\\sum_{t=1}^{T-1}a_tb_t(x_i)+ab(x_i), y_i)\\to min_{a,b}$\n",
    "Ищем вектор $u=(b(x_i))_{i=1}^l\\ из\\ R^l$ минимизирующий Q(a, b). Текущее положение вектора u: $u_{T-1}=(u_{T-1,i})_{i=1}^l$. Следующее положение вектора u: $u_{T}=(u_{T,i})_{i=1}^l$.\n",
    "\n",
    "\n",
    "### Параметрическая аппроксимация градиентного шага\n",
    "Градиентный метод минимизации $Q(u)\\to min,\\ u\\in \\mathbb{R}^l$: $u_0:=$ начальное приближение, $u_{T,i}:=u_{T,i-1}-\\alpha g_i,\\ i=1,...,l$, $g_i=\\mathcal{L}'(u_{T-1,i}, y_i)$ - компоненты вектора градиента, $\\alpha$ - градиентный шаг\n",
    "\n",
    "Добавление базового алгоритма $b_T$: $u_{T,i}:=u_{T,i-1}-\\alpha b_T(x_i),\\ i=1,...,l$\n",
    "\n",
    "Ищем такой базовый алгоритм $b_T$, чтобы вектор $(b_T(x_i))_{i=1}^l$ приближал вектор антиградиента $(-g_i)_{i=1}^l$: $b_T:=argmax_b\\sum_{i=1}^l(b(x_i)+g_i)^2$\n",
    "\n",
    "\n",
    "### Алгоритм градиентного бустинга\n",
    "Вход: обучающая выборка $X^l$, параметр Т\n",
    "Выход: базовые алгоритмы и их веса $\\alpha_tb_t,\\ t=1,...,T$\n",
    "1) инициализация: $u_i:=0,\\ i=1,...,l$   \n",
    "2) для всех t=1,...,T   \n",
    "3) найти базовый алгоритм, приближающий градиент $b_t:=argmin_b\\sum_{i=1}^l(b(x_i)+\\mathcal{L}'(u_i,y_i))^2$   \n",
    "4) решить задачу одномерной минимизации: $\\alpha_t:=argmin_{\\alpha>0}\\sum_{i=1}^l\\mathcal{L}(u_i+\\alpha b_t(x_i),y_i)$    \n",
    "5) обновить значения композиции на объектах выборки: $u_i:=u_i+\\alpha_tb_t(x_i),\\ i=1,...,l$  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг: модификации и эвристики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стохастический градиентный бустинг\n",
    "Рандомизация может повышать качество композиции за счет повышения различности базовых алгоритмов\n",
    "\n",
    "В шагах 3-5 использовать не всю выборку, а случайную подвыборку с повторениями как в бэггинге (см алгоритм выше)\n",
    "\n",
    "Преимущества:\n",
    "- улучшается качество\n",
    "- улучшается сходимость\n",
    "- уменьшается время обучения\n",
    "\n",
    "\n",
    "### Частный случай - алгоритм AdaBoost\n",
    "$\\mathcal{L}(b(x_i), y_i)=e^{-b(x_i)y_i}$ - экспоненциальная функция потерь, убывающая функция отступа $M_i=b(x_i)y_i$\n",
    "\n",
    "Преимущества:\n",
    "- для обучения $b_t$ на каждом шаге t решается стандартная задача минимизации взвешанного эмпирического риска\n",
    "- задача оптимизации $\\alpha_t$ решается аналитически\n",
    "\n",
    "Недостаток:\n",
    "- слишком чувствителен к выбросам из-за экспоненциального роста функции потерь при $M_i<0$\n",
    "\n",
    "\n",
    "### Градиентный бустинг над деревьями\n",
    "Решающее дерево имеет кусочно-постоянную функцию: $b(x)=\\sum_{t=1}^T\\alpha_t[x\\in\\Omega_t]$, T- число листьев, $\\Omega_t$- область t-го листа, $\\alpha_t$- прогноз в t-ом листе\n",
    "\n",
    "Каждый лист - базовый алгоритм $b_t(x)=[x\\in\\Omega_t]$, градиентным шагом определяется прогноз $\\alpha_t$ в t-ом листе: $\\alpha_t=argmin_{\\alpha>0}\\sum_{x_i\\in\\Omega_t}\\mathcal{L}(u_{t-1,i}+\\alpha, y_i)$. После определения всех $\\alpha_t$ можно добавить в композицию следующее дерево, оптимизировав его структору по среднеквадратичной ошибке\n",
    "\n",
    "\n",
    "Для некоторых функций потерь решение находится аналитически:\n",
    "- средний квадрат ошибок, $\\mathcal{L}(b, y)=(b-y)^2$: $\\alpha_t=\\frac{1}{\\Omega_t}\\sum_{x_i\\in \\Omega_t}(y_i-u_{t1-,i})$\n",
    "- средняя абсолютная ошибка, $\\mathcal{L}(b, y)=|b-y|$: $\\alpha_t=median_{x_i\\in \\Omega_t}{y_i-u_{t-1,i}}$\n",
    "\n",
    "В общем случае аналитического решения нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная модель нейрона МакКаллока-Питтса\n",
    "$f_j:X\\to \\mathbb{R}, j=1,...,n$ - числовые признаки, $x^j=f_j(x)$    \n",
    "$a(x, w)=\\sigma(\\langle w, x \\rangle)=\\sigma(\\sum_{j=1} ^nw_jf_j(x)-w_0),\\ w_0,...,w_n\\in\\mathbb{R}$ - веса признаков, $\\sigma(s)$ - функция активации\n",
    "\n",
    "\n",
    "### Часто используемые функции активации $\\sigma(z)$\n",
    "$\\theta(z)=[z\\leq0]$ - пороговая функция Хевисайда    \n",
    "$\\sigma(z)=(1+e^{-z})^{-1}$ - сигмоидная функция    \n",
    "$th(z)=2\\sigma(2z)-1$ - гиперболический тангенс    \n",
    "$ln(z+\\sqrt(z^2+1))$ - логарифмическая функция    \n",
    "$e^{-\\frac{z^2}{2}}$ - гаусовская функция   \n",
    "z - линейная функция\n",
    "\n",
    "\n",
    "### Линейные алгоритмы классификации и регрессии\n",
    "Задача классификации: $Y={\\pm1},\\ a(x, w)=sign\\langle w, x_i \\rangle$, $Q(w, X^l)=\\sum_{i=1}^l\\mathcal{L}(\\langle w, x_i \\rangle y_i)\\to min_w$\n",
    "\n",
    "Задача регрессии: $Y=\\mathbb{R},\\ a(x, w)=\\sigma(\\langle w, x_i \\rangle)$, $Q(w, X^l)=\\sum_{i=1}^l(\\sigma(\\langle w, x_i \\rangle)-y_i)^2\\to min_w$\n",
    "\n",
    "\n",
    "### Нейронная реализация логических функций\n",
    "Функции И, ИЛИ, НЕ от бинарных переменных $x^1$ и $x^2$:\n",
    "- $x^1 \\land x^2=[x^1+x^2-\\frac{3}{2}>0]$\n",
    "- $x^1 \\lor x^2=[x^1+x^2-\\frac{1}{2}>0]$\n",
    "- $\\neg x^1=[-x^1+\\frac{1}{2}>0]$\n",
    "\n",
    "\n",
    "### Логическая функция XOR (исключающее или)\n",
    "Функция $x^1 \\oplus x^2=[x^1\\neq x^2]$ не реализуема одним нейроном. Два способа реализации:\n",
    "- Добавление нелинейного признака: $x^1 \\land x^2=[x^1+x^2-2x^1x^2-\\frac{1}{2}>0]$\n",
    "- Сетью (двуслойной суперпозицией) функций И, ИЛИ, НЕ: $x^1 \\land x^2=[(x^1\\lor x^2)-(x^1\\land x^2)-\\frac{1}{2}>0]$\n",
    "\n",
    "\n",
    "### Представление функции нейросетью\n",
    "- Двухслойная сеть в $\\{0,1\\}^n$ позволяет реализовать произвольную булеву функцию \n",
    "- Двухслойная сеть в $\\mathbb{R}$ позволяет отделить произвольный выпуклый многогранник\n",
    "- Трехслойная сеть в $\\mathbb{R}$ позволяет отделить произвольную многогранную область, не обязательно выпуклую, и даже не обязательно связную\n",
    "- С помощью линейных операций и одной нелинейной фукнции активации $\\phi$ можно приблизить любую непрерывную функцию с любой желаемой точностью\n",
    "\n",
    "Практические рекомендации:\n",
    "- Двух-трех слоев обычно достаточно\n",
    "- Можно достраивать нейроны в произвольных местах сети по необходимости, вообще не заботясь о числе слоев\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети. Метод обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многослойная нейронная сеть\n",
    "Пусть $Y=\\mathbb{R}$, для простоты слоев только два: $a^m(x,w)=\\sigma_m(\\sum_{h=0}^Hw_{hm}\\sigma_h(\\sum_{j=0}^nw_{jh}f_j(x_i)))$, на входной слой подается n признаков, скрытый слой состоит из H нейронов, выходной слой состоит из М нейронов\n",
    "\n",
    "\n",
    "### Алгоритм SG (Stochastic Gradient)\n",
    "Вход: выборка $X^l$, темп обучения $\\eta$, темп забывания $\\lambda$   \n",
    "Выход: вектор весов $w\\equiv(w_{jh}, w_{hm})\\in \\mathbb{R}^{H(n+M+1)+M}$    \n",
    "Задача минимизации суммарных потерь: $Q(w):=\\sum_{i=1}^l\\mathcal{L}_i(w)\\to min_{w}$\n",
    "\n",
    "1) инициализировать веса $w_j, j=0,...,n$ и текущую оценку функционала $\\bar{Q}$   \n",
    "2) повторять    \n",
    "3) выбрать объект $x_i$ из $X^l$ случайным образом     \n",
    "4) вычислить потерю $\\mathscr{L}_i:=\\mathscr{L}_i(w)$   \n",
    "5) сделать градиентный шаг $w:=w-\\eta\\nabla\\mathscr{L}_i(w)$    \n",
    "6) оценить функционал $\\bar{Q}:=(1-\\lambda)Q+\\lambda\\mathscr{L}_i$  \n",
    "7) пока значение $\\bar{Q}$ и/или веса w не сойдутся\n",
    "\n",
    "\n",
    "### Задача дифференцирования суперпозиции функций\n",
    "Выходные значения сети $a^m(x_i),\\ m=1,...,M$ на объекте $x_i$: $a^m(x_i)=\\sigma_m\\sum_{h=0}^Hw_{hm}u^h(x_i),\\ u^h(x_i)=\\sigma_h\\sum_{j=0}^Jw_{jh}f_j(x_i)$\n",
    "\n",
    "Пусть для конкретности $\\mathcal{L}_i(w)$ - средний квадра ошибки: $\\mathcal{L}_i(w)=\\frac{1}{2}\\sum_{m=1}^{M}(a^m(x_i)-y_i^m)^2$\n",
    "\n",
    "Промежуточная задача: найти частные производные $\\frac{\\partial\\mathcal{L}_i(w)}{\\partial a^m},\\ \\frac{\\partial\\mathcal{L}_i(w)}{\\partial u^h}$\n",
    "\n",
    "\n",
    "### Быстрое вычисление градиента\n",
    "Промежуточная задача: частные производные $\\frac{\\partial\\mathcal{L}_i(w)}{\\partial a^m}=a^m(x_i)-y^m_i=\\epsilon_i^m$ - ошибка на выходном слое, $\\frac{\\partial\\mathcal{L}_i(w)}{\\partial u^h}=\\sum_{m=1}^M(a^m(x_i)-y_i^m)\\sigma_m'w_{hm}=\\sum_{m_1}^M\\epsilon_i^m\\sigma_m'w_{hm}=\\epsilon_i^h$ - ошибка на скрытом слое\n",
    "\n",
    "Похоже, что $\\epsilon_i^h$ вычисляется по $\\epsilon_i^m$ путем его пропускания через сеть в обраном направлении\n",
    "\n",
    "\n",
    "### Быстрое вычисление градиента\n",
    "Имея частные производные $\\mathcal{L}_i(w)$ по $a^m$ и $u^h$, легко выписать градиент $\\mathcal{L}_i(w)$ по весам w:  \n",
    "$\\frac{\\partial\\mathcal{L}_i(w)}{\\partial w_{hm}}=\\frac{\\partial\\mathcal{L}_i(w)}{\\partial a^m}\\frac{\\partial a^m}{\\partial w_{hm}}=\\epsilon_i^m\\sigma_m'u^h(x_i),\\ m=1,...,M,\\ h=0,...,H$,    \n",
    "$\\frac{\\partial\\mathcal{L}_i(w)}{\\partial w_{jh}}=\\frac{\\partial\\mathcal{L}_i(w)}{\\partial u^h}\\frac{\\partial u^h}{\\partial w_{jh}}=\\epsilon_i^h\\sigma_h'f_j(x_i),\\ h=1,...,M,\\ j=0,...,n$\n",
    "\n",
    "\n",
    "### Алгоритм обратного распространения ошибки BackProp:\n",
    "1) инициировать веса $w_{jh},\\ w_{hm}$   \n",
    "2) повторять   \n",
    "3) выбрать объект $x_i$ из $X^l$  \n",
    "4) прямой ход:  \n",
    "    -$u_i^h=\\sigma_h(\\sum_{j=0}^Jw_{jh}x^j_i),\\ h=1,...,H$  \n",
    "    -$a^m_i=\\sigma_m(\\sum_{h=0}^Hw_{hm}u_i^h),\\ \\epsilon_i^m=a_i^m-y_i^m,\\ m=1,...,M$  \n",
    "    -$\\mathcal{L}_i=\\sum_{m=1}^M(\\epsilon^m_i)^2$  \n",
    "5) обратный ход: $\\epsilon_i^h=\\sum_{m=1}^M\\epsilon_i^m\\sigma_m'w_{hm},\\ h=1,...,H$    \n",
    "6) градиентный шаг:    \n",
    "    -$w_{hm}=w_{hm}-\\eta\\epsilon^m_i\\sigma_m'u_i^h,\\ h=0,...,H,\\ m=1,...,M$     \n",
    "    -$w_{jh}=w_{jh}-\\eta\\epsilon^h_i\\sigma_h'x^j_i,\\ j=0,...,n,\\ h=1,...,H$   \n",
    "7) $Q=(1-\\lambda)Q+\\lambda\\mathcal{L}_i$   \n",
    "8) пока Q не стабилизируется\n",
    "\n",
    "Преимущества:\n",
    "- быстрое вычисление градиента\n",
    "- метод легко обобщается на любые $\\sigma,\\ \\mathcal{L}$\n",
    "- возможно динамическое обучение\n",
    "- на сверхбольших выборках не обязательно брать все значения\n",
    "- возможность расспараллеливания\n",
    "\n",
    "Недостатки:\n",
    "- возможна медленная сходимость\n",
    "- застревание в локальных минимумах\n",
    "- проблема паралича сети (горизонтальные ассимптоты $\\sigma$)\n",
    "- проблема переобучения\n",
    "- подбор комплекса эвристик является искусством"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети. Стандартные эвристики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартные эвристики для метода стохастического градиента\n",
    "Эвристики:\n",
    "- инициализация весов\n",
    "- порядок предъявления объектов\n",
    "- оптимизация величины градиентного шага\n",
    "- регулярзация\n",
    "\n",
    "Дополнительные проблемы при обучении:\n",
    "- выбор функции активации в каждом нейроне\n",
    "- выбор числа слоев и числа нейронов\n",
    "- выбор значисых связей\n",
    "\n",
    "\n",
    "### Ускорение сходимости\n",
    "1) Начальное приближение - послойное обучение сети. Нейроны настраиваются как отдельные линейные алгоритмы, тем самым обеспечивая различность нейронов: по случайной подвыборке, по случайному подмножеству входов, из различных случайных приближений\n",
    "2) Выбивание из локальных минимумов\n",
    "3) Адаптивный градиентный шаг\n",
    "4) Метод сопряженных градиентов и chunking - разбиение суммы $Q(w)=\\sum_{i=1}^l\\mathcal{L}_i(w)$ по подмножествам объектов\n",
    "\n",
    "\n",
    "### Диагональный метод Левенберга-Марквардта\n",
    "Метод Ньютона-Рафсона (второго порядка): $w=w-\\eta(\\mathcal{L}_i''(w))^{-1}\\mathcal{L}_i'(w)$, где $\\mathcal{L}_i''(w)=\\frac{\\partial^2\\mathcal{L}_i(w)}{\\partial w_{jh}\\partial w_{j'h'}}$ - гессианн размера $(H(n+M+1)+M)^2$\n",
    "\n",
    "Эвристика: считаем, что гессиан диагонален $w_{jh}=w_{jh}-\\eta(\\frac{\\partial^2\\mathcal{L}_i(w)}{\\partial w_{jh}^2}+\\mu)^{-1}\\frac{\\partial\\mathcal{L}_i(w)}{\\partial w_{jh}}$, $\\eta$ - темп обучения, $\\mu$ - параметр, предотвращающий обнуление знаменателя\n",
    "\n",
    "Отношение $\\frac{\\eta}{\\mu}$ темп обучения на ровных участках функционала $\\mathcal{L}_i(w)$ где вторая производная обнуляется\n",
    "\n",
    "\n",
    "### Динамическое наращивание сети\n",
    "Обучение прри заведомо недостаточном числе нейронов H. После стабилизации Q(w) - добавление нового нейрона и его инициализация путем обучения:\n",
    "- по случайной подвыборке\n",
    "- по объектам с наибольшим значением потерь\n",
    "- по случайному подмножеству входов\n",
    "- из различных начальных приближений   \n",
    "Продолжать итерации BackProp\n",
    "\n",
    "В таком варианте общее время больше примерно в 2 раза чем если бы обучение происходило сразу по нужному числу нейронов. Полезная информация не теряется сетью при добавлении новых нейронов\n",
    "\n",
    "\n",
    "### Прореживание сети (Optimal Brain Damage)\n",
    "Пусть w - локальный минимум Q(w), тогда Q(w) можно аппроксимировать квадратичной формой: $Q(w+\\delta)=Q(w)+\\frac{1}{2}\\delta^TQ''(w)\\delta+o(||\\delta||^2)$, $Q''(w)=\\frac{\\partial^2 Q(w)}{\\partial w_{jh}\\partial w_{j'h'}}$- гессиан, размера $(H(n+M+1)+M)^2$\n",
    "\n",
    "Эвристика: считаем что гессиан диагонален, тогда $\\delta^TQ''(w)\\delta=\\sum_{j=1}^n\\sum_{h=1}^H \\delta^2_{jh}\\frac{\\partial^2 Q(w)}{\\partial w_{jh}^2}+\\sum_{h=0}^H\\sum_{m=0}^M\\delta^2_{hm}\\frac{\\partial^2 Q(w)}{\\partial w_{hm}^2}$\n",
    "\n",
    "\n",
    "Обнуляем вес: $w_{jh}+\\delta_{jh}=0$\n",
    "\n",
    "Значимость веса $w_{jh}$ - это изменение функционала Q(w) при его обнулениии: $S_{jh}=w_{jh}^2\\frac{\\partial^2 Q(w)}{\\partial w_{jh}^2}$\n",
    "\n",
    "В BackProp вычислять вторые производные $\\frac{\\partial^w Q}{\\partial w_{jh}^2},\\ \\frac{\\partial^w Q}{\\partial w_{hm}^2}$\n",
    "\n",
    "Если процесс минимизации Q(w) пришел в минимум, то \n",
    "- упорядочить веса по убыванию $S_{jh}$\n",
    "- удалить N связей с наименьшей значимостью\n",
    "- снова запустиь BackProp\n",
    "\n",
    "Если $Q(w, X^l),\\ Q(w,X^k)$ существенно ухудшился, то вернуть последние удаленные связи и выйти\n",
    "\n",
    "Отбор признаков: с помощью OBD аналогично. суммарная значимсоть признаков: $S_j=\\sum_{h=1}^H S_{jh}$\n",
    "\n",
    "Результат постепенного прореживания обычно лучше, чем BackProp изначально прореженной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
