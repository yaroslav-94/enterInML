{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи кластеризации\n",
    "Кластеризация - это обучение без учителя\n",
    "Дано:\n",
    "- Х - пространство объектов  \n",
    "- $X^l=\\{x_i\\}_{i=1}^l$ - обучающая выборка   \n",
    "- $\\rho:X*X\\to[0,\\infty)$ - функция расстояния между объектами  \n",
    "\n",
    "Найти:\n",
    "- $y_i\\in Y$ - метки кластеров объектов\n",
    "- каждый кластер состоит из близких объектов\n",
    "- объекты разных кластеров существенно различны\n",
    "\n",
    "Решение задачи кластеризации принципиально неоднозначно:\n",
    "- разлчиные критерии качества кластеризации\n",
    "- различные эвричтические методы кластеризации\n",
    "- различные варианты функции расстояния $\\rho$\n",
    "\n",
    "\n",
    "### Типы кластерных структур\n",
    "- внутрикластерные расстояния, как правило, меньше межкластерных\n",
    "- кластеры с центором\n",
    "- кластеры могут соединяться перемычками\n",
    "- кластеры могут накладываться на разреженный фон из редко расположенных объектов\n",
    "- ленточные кластеры\n",
    "- перекрывающиеся кластеры\n",
    "- кластеры могут образовываться не по сходству, а по иным типам регулярноностей\n",
    "- кластеры могут отсутствовать\n",
    "\n",
    "\n",
    "### Метод k-means (k-средних)\n",
    "Объекты $x_i$ задаются векторамии признаков $(f_1(x_i),...,f_n(x_i))$\n",
    "Вход: $X^l$ - обучающая выборка параметр k  \n",
    "Выход: центры кластеров $\\mu_y, y\\in Y$   \n",
    "1) начальное приближение центров $\\mu_y, y\\in Y$   \n",
    "2) повторять   \n",
    "3) отнести каждый $x_i$ к ближайшему центру: $y_i:=argmin_{y\\in Y}\\rho(x_i, \\mu_y),\\ i=1,...,l$   \n",
    "4) вычислить новые оложения ценров: $\\mu_{yj}=\\frac{\\sum_{i=1}^l[y_i=y]f_j(x_i)}{\\sum_{i=1}^l[y_i=y]},\\ y\\in Y,\\ j=1,...,n$  \n",
    "5) пока $y_i$ не перестанут изменяться  \n",
    "\n",
    "\n",
    "### Мягкий вариант k-средних\n",
    "Вход: $X^l$ - обучающая выборка параметр k  \n",
    "Выход: центры кластеров $\\mu_y, y\\in Y$   \n",
    "1) начальное приближение центров $\\mu_y,\\ y\\in Y,\\ w_y=\\frac{1}{|Y|}$  \n",
    "2) повторять  \n",
    "3) оценить близость каждого $x_i$ ко всем центрам: $g_{yi}=w_y exp(-\\frac{1}{2}\\rho^2(x_i, \\mu_y)),\\ i=1,...,l,\\ y\\in Y$, $g_{iy}=\\frac{g_{iy}}{\\sum_{z\\in Y}g_{iz}}$   \n",
    "4) отнести каждый $x_i$ к ближайшему центру: $y_i:=argmin_{y\\in Y}g_{iy},\\ i=1,...,l$   \n",
    "5) новые положения центров и мощности кластеров: $\\mu_{yj}=\\frac{1}{lw_y}\\sum_{i=1}^lg_{iy}f_j(x_i),\\ w_y=\\frac{1}{l}\\sum_{i=1}^lg_{iy},\\ y\\in Y,\\ j=1,...,n$   \n",
    "6) пока $g_{iy}$ не перестанут меняться  \n",
    "\n",
    "\n",
    "### Формирование начального приближения для k-средних\n",
    "Вход: $X^l$ - обучающая выборка, параметры $q, \\delta, k$   \n",
    "Выход: $U \\subset X^l$ - начальное приближение центров $\\mu_y, y\\in Y$\n",
    "1) среднее расстояние до q ближайших соседей: $R_i=\\frac{1}{q}\\sum_{j=1}^q\\rho(x_i,x_i^{(j)})$ для всех $i=1,...,l$, где $x_i^{(j)}$ - j-й ближайший сосед объекта $x_i$   \n",
    "2) отбросить шумовые объекты: $X'=\\{x_i\\in X^l | R_i\\leq\\Delta\\}$ при $\\Delta: |X'|=(1-\\delta)l$   \n",
    "3) выбрать пару самых удаленных объектов: $U=argmax_{x,x'\\in X'}\\rho(x,x')$, далее последовательно присоединять к U по одному объекту, самому удаленному от уже выбранных   \n",
    "4) повторять (k-2) раз $U=U\\cup argmax_{x\\in X'}min_{u\\in U}\\rho(x,u)$\n",
    "\n",
    "\n",
    "### Недостатки k-средних\n",
    "- чувствительность к выбору начального приближения\n",
    "- необходимость задавать k\n",
    "\n",
    "Способы устранения недостатков:\n",
    "- эвристики для выбора начального приближения\n",
    "- мягкая кластеризация\n",
    "- мультистарт: несколько случайных инициализаций, выбоор лучшей кластеризации по функционалу качества\n",
    "- быстрые алгоритмы (k++б сэмплирование)\n",
    "- варьирование числа кластеров k в ходе итерацийй\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иерархическая кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи кластеризации\n",
    "Кластеризация - это обучение без учителя\n",
    "Дано:\n",
    "- Х - пространство объектов  \n",
    "- $X^l=\\{x_i\\}_{i=1}^l$ - обучающая выборка   \n",
    "- $\\rho:X*X\\to[0,\\infty)$ - функция расстояния между объектами  \n",
    "\n",
    "Найти:\n",
    "- $y_i\\in Y$ - метки кластеров объектов\n",
    "- каждый кластер состоит из близких объектов\n",
    "- объекты разных кластеров существенно различны\n",
    "\n",
    "Необходимо научиться определять число кластеров\n",
    "\n",
    "\n",
    "### Агломеративная иерархическая кластеризация\n",
    "Алгоритм Ланса-Уильямса основан на оценивании расстояний R(U, V) между парами кластеров U, V  \n",
    "1) сначала все кластеры одноэлементные: $t=1,\\ C_t=\\{\\{x_1\\},..., \\{x_l\\}\\},\\ R(\\{x_i\\}, \\{x_j\\})=\\rho(x_i, x_j)$  \n",
    "2) для всех t=2,...,l (t- номер итерации)\n",
    "3) найти в $C_{t-1}$ два ближайших кластера: $(U, V)=argmin_{U\\neq V} R(U,V),\\ R_t=R(U,V)$  \n",
    "4) слить их в один кластер: $W=U\\cup V,\\ C_t=C_{t-1}\\cup \\{W\\}|\\{U,V\\}$  \n",
    "5) для всех $S\\in C_t$  \n",
    "6) вычислить R(W, S) по формуле Ланса-Удьямса\n",
    "\n",
    "\n",
    "### Формула Ланса-Уильямса\n",
    "Определение расстояния R(W, S) между кластерами $W=U\\cup V$ и S, зная расстояния R(U, S), R(V, S), R(U, V)\n",
    "\n",
    "Формула обобщающая большинство разумных способов определения этого расстояния: $R(U\\cup V, S)=\\alpha_U R(U, S)+\\alpha_V R(V, S)+\\beta R(U, V) + \\gamma |R(U,S)-R(V,S)|$, $\\alpha_U,\\alpha_V, \\beta, \\gamma$ - числовые параметры\n",
    "\n",
    "\n",
    "### Частные случаи формулы Ланса-Уильямса\n",
    "1) Расстояние ближайшего соседа  \n",
    "2) Расстояние дальнего соседа  \n",
    "3) Груповое среднеее расстояние  \n",
    "4) Расстояние между центрами  \n",
    "5) Расстояние Уорда: $R^У(W,S)=\\frac{|S||W|}{|S|+|W|}\\rho^2(\\sum_{w\\in W}\\frac{w}{|W|}, \\sum_{s\\in S}\\frac{s}{|S|}),\\ \\alpha_U=\\frac{|S|+|U|}{|S|+|W|},\\ \\alpha_V=\\frac{|S|+|V|}{|S|+|W|},\\ \\beta=\\frac{-|S|}{|S|+|W|}, \\gamma=0$  \n",
    "\n",
    "\n",
    "### Основные свойства иерархической кластеризации\n",
    "- Монотонность: дендрограмма не имеет самопересечений, при каждом слиянии расстояние между объединяемыми кластерами только увеличивается: $R_2\\leq R_3\\leq ... \\leq R_l$. достаточное условие монотонности: $\\alpha_U\\geq0,\\ \\alpha_V\\geq0,\\ \\alpha_U+\\alpha_V+\\beta\\geq0,\\ min\\{\\alpha_U, \\alpha_V\\}+\\gamma\\geq0$    \n",
    "- Сжимающее расстояние: $R_t\\leq\\rho(\\mu_U,\\mu_V),\\ \\forall t$  \n",
    "- Растягивающее расстояние: $R_t\\geq\\rho(\\mu_U,\\mu_V),\\ \\forall t$  \n",
    "$R^У(W,S)$ - монотонно и растягивающе\n",
    "\n",
    "\n",
    "### Рекомендации\n",
    "- использовать расстояние Уорда\n",
    "- строить несколько вариантов и выбирать лучший по дендрограмме\n",
    "- определение числа кластеров - по максимуму $|R_{t+1}-R_t|$, тогда результирующее множество кластеров $C_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нелинейные методы понижения размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нелинейное понижение размерности\n",
    "Дано: $\\{x_1,...,x_l\\}$ - выборка объектов\n",
    "Задача: отобразить все объекты выборки в пространство малой размерности $x_i \\mapsto \\tilde{x_i}\\in \\mathbb{R}^d$\n",
    "Требования к маломерному представлению: \n",
    "- должно хорошо отображать структуру данных в исходном пространстве\n",
    "- сохраняет интересующие закономерности в данных\n",
    "\n",
    "\n",
    "### Многомерное шкалирование (MDS)\n",
    "Гипотеза: хорошее малоразмерное представление сохраняет попарные расстояния между объектами   \n",
    "$d_{ij}$ - расстояние мужду $x_i,x_j$ (признаковые описания не нужны - достаточно расстояний)   \n",
    "$d_{ij}=||\\tilde{x_i}-\\tilde{x_j}||$ - евклидово расстояние между маломерными представлениями   \n",
    "Ищем представление аппроксимирующее $d_{ij}$: $\\sum_{i<j}^l(||\\tilde{x_i}-\\tilde{x_j}||-d_{ij})^2\\to min_{(\\tilde{x_i})_{i=1}^l\\subset\\mathbb{R}^d}$  \n",
    "Оптимизация: алгоритм SMACOF\n",
    "\n",
    "\n",
    "### SNE: Stochastic Neighbor Embedding\n",
    "- В точности воспроизвести расстояния - слишком сложно\n",
    "- Достаточно сохранения пропорций: $\\rho(x_1,x_2)=c\\rho(x_1,x_3)\\implies\\rho(\\tilde{x_1}, \\tilde{x_2})=c\\rho(\\tilde{x_1}, \\tilde{x_3})$\n",
    "- Описание объектов с нормированными расстояниями до остальных объектов: $p(x_j|x_i)=\\frac{exp(||x_i-x_j||^2/2\\sigma^2)}{\\sum_{k\\neq i}exp(||x_i-x_k||^2/2\\sigma^2)},\\ q(\\tilde{x_j}|\\tilde{x_i})=\\frac{exp(||\\tilde{x_i}-\\tilde{x_j}||^2/2\\sigma^2)}{\\sum_{k\\neq i}exp(||\\tilde{x_i}-\\tilde{x_k}||^2)}$\n",
    "- Минимизируем разницу между распределениями расстояний (мера - дивергенция Кульбака-Лейблера): $\\sum_{i=1}^l\\sum_{j\\neq i}p(x_j|x_i)log\\frac{p(x_j|x_i)}{p(\\tilde{x_j}|\\tilde{x_i})}\\to min_{(\\tilde{x_i})_{i=1}^l\\subset\\mathbb{R}^d}$\n",
    "\n",
    "\n",
    "\n",
    "### t-SNE\n",
    "t-Distributed Stochastic Neighbor Embedding - развитие SNE:\n",
    "- чем выше размерность пространства, тем меньше расстояние между парами точек отличаются друг от друга\n",
    "- невозможно воспроизвести это свойство в двух- или трехмерном пространстве\n",
    "- значит нужно меньше штрафовать за увеличение пропорций в малоразмерном пространстве\n",
    "- измененное распределение: $q(\\tilde{x_j}|\\tilde{x_i})=\\frac{(1+||\\tilde{x_i}-\\tilde{x_j}||^2)^{-1}}{\\sum_{k\\neq i}(1+||\\tilde{x_i}-\\tilde{x_k}||^2)^{-1}}$\n",
    "- такая визуализация позволяет увидеть внутреннюю структуру данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи частичного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи частичного обучения\n",
    "Дано: \n",
    "- множество объектов Х, множество классов Y; \n",
    "- $X^l=\\{x_1,...,x_l)\\}$ - размеченная выборка\n",
    "- $X^k=\\{x_{l+1},..., x_{l+k}\\}$ - неразмеченная выборка\n",
    "\n",
    "Два варианта постановки задачи:\n",
    "- частичное обучение (SSL-semi-supervised learning): построить алгоритм классификации $a: X\\to Y$\n",
    "- трансдуктивное обучение: зная все $\\{x_{l+1},...,x_{l+k}\\}$ полкчить метки $\\{y_{l+1},..., y_{l+k}\\}$\n",
    "\n",
    "\n",
    "### SSL не сводится к задаче классификации и кластеризации\n",
    "\n",
    "\n",
    "### Метод self-training\n",
    "Пусть $\\mu:X^l\\to a$ - произвольный метод обучения; классификаторы имеют вид $a(x)=argmax_{y\\in Y}\\Gamma_y(x)$\n",
    "\n",
    "Псевдоотступ - степень уверенности классификации $a_i=a(x_i)$: $M_i(a)=\\Gamma_{a_i}(x_i)-max_{y\\in Y|a_i}\\Gamma_y(x_i)$\n",
    "\n",
    "Алгоритм self-training - обертка над методом $\\mu$:\n",
    "1) $Z=X^l$   \n",
    "2) пока |Z|<l+k  \n",
    "3) $a=\\mu(Z)$  \n",
    "4) $\\Delta=\\{x_i\\in X^k\\setminus Z| M_i(a)\\geq M_0\\}$  \n",
    "5) $y_i=a(x_i)$ для всех $x_i\\in \\Delta$  \n",
    "6) $Z=Z \\cup \\Delta$  \n",
    "\n",
    "$M_0$ - определяется например из условия $|\\Delta|=0.05k$\n",
    "\n",
    "\n",
    "### Метод co-training\n",
    "Пусть $\\mu_1:X^l\\to a_1$, $\\mu_2:X^l\\to a_2$ два существенно различных метода обучения, использующих \n",
    "- либо различные наборы признаков\n",
    "- либо разные парадигмы обучения\n",
    "- либо разные источники данных $X_1 ^{l_1}, X_1 ^{l_1}$\n",
    "\n",
    "1) $Z_1=X_1 ^{l_1},\\ Z_2=X_1 ^{l_1}$   \n",
    "2) пока $|Z_1\\cup Z_2|<l+k$    \n",
    "3) $a_1=\\mu_1(Z_1)$, $\\Delta_1=\\{x_i\\in X^k\\setminus Z_1\\setminus Z_2| M_i(a_1)\\geq M_{01}\\}$   \n",
    "4) $y_i=a(x_i)$ для всех $x_i\\in \\Delta_1$  \n",
    "5) $Z_2=Z_2 \\cup \\Delta_1$  \n",
    "6) $a_2=\\mu_2(Z_2)$, $\\Delta_2=\\{x_i\\in X^k\\setminus Z_1\\setminus Z_2| M_i(a_2)\\geq M_{02}\\}$     \n",
    "7) $y_i=a(x_i)$ для всех $x_i\\in \\Delta_2$   \n",
    "8) $Z_1=Z_1 \\cup \\Delta_2$\n",
    "\n",
    "\n",
    "### Метод co-learning\n",
    "Пусть $\\mu_t:X^l\\to a_t$ - разные методы обучения, $t=1,...,T$.\n",
    "Алгоритм co-learning - это self-training для композиции - простого голосования базовых алгоритмов $a_1,...,a_T$: $a(x)=argmax_{y \\in Y}\\Gamma_y(x),\\ \\Gamma_y(x)=\\sum_{t=1}^T[a_t(x)=y]$, тогда $M_i(a)$ - степень уверенности классификации $a(x_i)$\n",
    "\n",
    "1) $Z=X^l$   \n",
    "2) пока $|Z|<l+k$   \n",
    "3) $a=\\mu(Z)$    \n",
    "4) $\\Delta=\\{x_i\\in X^k\\setminus Z| M_i(a)\\geq M_0\\}$    \n",
    "5) $y_i=a(x_i)$ для всех $x_i\\in \\Delta$    \n",
    "6) $Z_1=Z_1 \\cup \\Delta$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение кластеризации в решении задач частичного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи частичного обучения\n",
    "Дано: \n",
    "- множество объектов Х, множество классов Y; \n",
    "- $X^l=\\{x_1,...,x_l)\\}$ - размеченная выборка\n",
    "- $X^k=\\{x_{l+1},..., x_{l+k}\\}$ - неразмеченная выборка\n",
    "\n",
    "Два варианта постановки задачи:\n",
    "- частичное обучение (SSL-semi-supervised learning): построить алгоритм классификации $a: X\\to Y$\n",
    "- трансдуктивное обучение: зная все $\\{x_{l+1},...,x_{l+k}\\}$ полкчить метки $\\{y_{l+1},..., y_{l+k}\\}$\n",
    "\n",
    "\n",
    "### Кластеризация как задача дискретной оптимизации\n",
    "Пусть $\\rho(x,x')$ - функция расстояния между объектами. Веса на парах объектов (близости): $w_{ij}=exp(-\\beta \\rho(x_i, x_j))$, где $\\beta$ - параметр\n",
    "\n",
    "Задача кластеризации: $\\sum_{i=1}^{l+k}\\sum_{j=i+1}^{l+k} w_{ij}[a_i\\neq a_j]\\to min_{a_i \\in Y}$\n",
    "\n",
    "Задача частичного обучения: $\\sum_{i=1}^{l+k}\\sum_{j=i+1}^{l+k} w_{ij}[a_i\\neq a_j] + \\lambda\\sum_{i=1}^l[a_i\\neq y_i]  \\to min_{a_i \\in Y}$, $\\lambda$ - параметр\n",
    "\n",
    "\n",
    "### Алгоритм КНП: кластеризация\n",
    "Задано число кластеров К\n",
    "\n",
    "Графовый алгоритм КНП (кратчайший незамкнутый путь)\n",
    "1) найти пару вершин $(x_i, x_j)\\in X^{k+l}$ с наименьшим $\\rho(x_i,y_j)$ и соединить их ребром   \n",
    "2) пока в выборке остаются изолированные точки   \n",
    "3) найти изолированную точку, ближайшую к некоторой неизолированной   \n",
    "4) соединить эти две точки ребром  \n",
    "5) удалить К-1 самых длинных ребер  \n",
    "  \n",
    "\n",
    "### Алгоритм КНП: частичное обучение\n",
    "Задано число кластеров К\n",
    "\n",
    "Графовый алгоритм КНП (кратчайший незамкнутый путь)\n",
    "1) найти пару вершин $(x_i, x_j)\\in X^{k+l}$ с наименьшим $\\rho(x_i,y_j)$ и соединить их ребром   \n",
    "2) пока в выборке остаются изолированные точки   \n",
    "3) найти изолированную точку, ближайшую к некоторой неизолированной   \n",
    "4) соединить эти две точки ребром  \n",
    "5) пока есть путь между двумя вершинами разных классов  \n",
    "6) удалить самое длинное ребро на этом пути\n",
    "\n",
    "\n",
    "### Алгоритм Ланса-Уильямса: частичное обучение\n",
    "\n",
    "Алгоритм Ланса-Уильямса основан на оценивании расстояний R(U, V) между парами кластеров U, V  \n",
    "1) сначала все кластеры одноэлементные: $t=1,\\ C_t=\\{\\{x_1\\},..., \\{x_l\\}\\},\\ R(\\{x_i\\}, \\{x_j\\})=\\rho(x_i, x_j)$  \n",
    "2) для всех t=2,...,l (t- номер итерации)\n",
    "3) найти в $C_{t-1}$ два ближайших кластера: $(U, V)=argmin_{U\\neq V} R(U,V),\\ R_t=R(U,V)$, при условии, что в $U \\cup V$ нет объектов с разными метками  \n",
    "4) слить их в один кластер: $W=U\\cup V,\\ C_t=C_{t-1}\\cup \\{W\\}|\\{U,V\\}$  \n",
    "5) для всех $S\\in C_t$  \n",
    "6) вычислить R(W, S) по формуле Ланса-Удьямса: $R(U\\cup V, S)=\\alpha_U R(U, S)+\\alpha_V R(V, S)+\\beta R(U, V) + \\gamma |R(U,S)-R(V,S)|$\n",
    "\n",
    "\n",
    "### Метод k-means (k-средних): частичное обучение\n",
    "Объекты $x_i$ задаются векторамии признаков $(f_1(x_i),...,f_n(x_i))$\n",
    "Вход: $X^l$ - обучающая выборка параметр k  \n",
    "Выход: центры кластеров $\\mu_y, y\\in Y$   \n",
    "1) начальное приближение центров $\\mu_y, y\\in Y$   \n",
    "2) повторять   \n",
    "3) отнести каждый $x_i\\in X^k$ к ближайшему центру: $y_i:=argmin_{y\\in Y}\\rho(x_i, \\mu_y),\\ i=l+1,...,l$   \n",
    "4) вычислить новые оложения ценров: $\\mu_{yj}=\\frac{\\sum_{i=1}^l[y_i=y]f_j(x_i)}{\\sum_{i=1}^l[y_i=y]},\\ y\\in Y,\\ j=1,...,n$  \n",
    "5) пока $y_i$ не перестанут изменяться  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение классификации в решении задач частичного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи частичного обучения\n",
    "Дано: \n",
    "- множество объектов Х, множество классов Y; \n",
    "- $X^l=\\{x_1,...,x_l)\\}$ - размеченная выборка\n",
    "- $X^k=\\{x_{l+1},..., x_{l+k}\\}$ - неразмеченная выборка\n",
    "\n",
    "Два варианта постановки задачи:\n",
    "- частичное обучение (SSL-semi-supervised learning): построить алгоритм классификации $a: X\\to Y$\n",
    "- трансдуктивное обучение: зная все $\\{x_{l+1},...,x_{l+k}\\}$ полкчить метки $\\{y_{l+1},..., y_{l+k}\\}$\n",
    "\n",
    "\n",
    "### SVM: классификация\n",
    "Обучающая выборка $X^l=(x_i, y_i)_{i=1} ^l$     \n",
    "$x_i$ - объекты, векторы из множества $X=\\mathcal{R}^n$     \n",
    "$y_i$ - метки классов, элементы множества $Y=\\{-1, +1\\}$     \n",
    "$M_i(w, w_0)=(\\langle x, w \\rangle-w_0)y_i$ - отступ    \n",
    "$a(x, w, w_0)=sign(\\langle x, w \\rangle-w_0),\\ x,w\\in \\mathbb{R}^n,\\ w_0\\in \\mathbb{R}$    \n",
    "\n",
    "Задача обучения весов $w_0,\\ w$ по размеченной выборке: $Q(w, w_0)= \\sum_{1 \\leq i \\leq}(1-M_i()w, w_0)_+ + \\frac{1}{2C}||w||^2 \\to min_{w, w_0}$\n",
    "\n",
    "Функция $\\mathcal{L}(M)=(1-M_i()w, w_0)_+$ штрафует за уменьшение отступа, штрафует за попадание в зазор, $|M_i|$ не зависит от $y_i$ и определен на неразмеченных объектах\n",
    "\n",
    "\n",
    "### Trunsductive SVM: частичное обучение\n",
    "Обучение весов $w_0,\\ w$ по частично размеченной выборке: $Q(w, w_0)= \\sum_{1 \\leq i \\leq}(1-M_i()w, w_0)_+ + \\frac{1}{2C}||w||^2 + \\gamma\\sum_{i=l+1}^{l+k}(1-|M_i(w,w_0)|)_+ \\to min_{w, w_0}$\n",
    "Достоинства TSVM: \n",
    "- можно использовать ядра\n",
    "- имеются эффективные реализации для больших данных\n",
    "\n",
    "Недостатки TSVM:\n",
    "- решение неустойчиво если нет области разреженности\n",
    "- требуется настройка параметров $C,\\ \\gamma$\n",
    "\n",
    "\n",
    "### Многоклассовая логистическая регрессия\n",
    "Линейный классификатор на конечном множестве классов |Y|: $a(x)=argmax_{y\\in Y}\\langle w_y, x \\rangle,\\ x,w_y\\in \\mathbb{R}^n$\n",
    "\n",
    "Вероятность того, что объект $x_i$ относится к классу y: $P(y|x_i,w)=\\frac{exp(\\langle w_y, x_i \\rangle)}{\\sum_{c\\in Y}exp(\\langle w_c, x_i \\rangle)}$\n",
    "\n",
    "Задача максимизации регуляризованного правдоподобия: $Q(w)=\\sum_{i=1}^l log P(y|x_i,w)-\\frac{1}{2C}\\sum_{y\\in Y}||w_y||^2\\to max_{w}$. Оптимизация Q(w) методом стохастического градиента\n",
    " \n",
    "\n",
    "### Логистическая регрессия с частичным обучением\n",
    "Теперь учтем неразмеченные данные $X^k={x_{l+1},...,x_{k+l}}$\n",
    "Пусть $b_j(x)$ - бинарные признаки, j=1,...,m\n",
    "Оценим вероятности $P(y|b_j(x),w)=1$ двумя способами:\n",
    "- эмпирическая оценка по размеченным данным $X^l$: $\\hat{p_j(x)}=\\frac{\\sum_{i=1}^lb_j(x_i)[y_i=y]}{\\sum_{i=1}^lb_j(x_i)}$\n",
    "- оценка по неразмеченным данным $X^k$ и линейной модели: $p_j(y,w)=\\frac{\\sum_{i=l+1}^{l+k}b_j(x_i)P(y|x_i,w)}{\\sum_{i=l+1}^{l+k}b_j(x_i)}$\n",
    "\n",
    "Минимизируем расстояние между $\\hat{p_j(x)}$ и $p_j(y,w)$, в качестве расстояния между распределениями возьмем дивергенцию Кульбака-Лейблера\n",
    "\n",
    "\n",
    "### Построение функционала качества\n",
    "Минимизация KL-дивергенции между $\\hat{p_j(x)}$ и $p_j(y,w)$: $KL(\\hat{p_j(x)}||p_j(y,w))=\\sum_y\\hat{p_j(x)}log\\frac{\\hat{p_j(x)}}{p_j(y,w)}\\to min_{w}$\n",
    "\n",
    "Вычитая сумму KL-дивергенции по всем признакам j=1,...,m из функционала регуляризованного правдоподобия Q(w): $\\hat{Q}(w)=\\sum_{i=1}^llogP(y_i|x_,w)-\\frac{1}{2C}\\sum_{y\\in Y}||w_y||^2+\\gamma\\sum_{j=1}^{l+k}\\sum_{y\\in Y}\\hat{p_j}(y)log\\frac{\\sum_{i=l+1}^{l+k}b_j(x)P(y|x_i,w)}{\\sum_{i=l+1}^{l+k}b_j(x_i)}\\to max_w$, $\\gamma$ коэффициент регуляризации\n",
    "\n",
    "\n",
    "### Особенности регуляризации для частичного обучения\n",
    "- оптимизация $\\tilde{Q}(w)$ - методом стохастического градиента\n",
    "- возможные варианты задания переменных $b_j$:\n",
    "    - $b_j(x)=1$, тогда $P(y|b_j(x)=1)$ - априорная вероятность класса y - хорошо подходит для задач с несбалансированными классами\n",
    "    - $b_j(x)=$[термин j содержится в х] - для задач классификации и каталогизации текстов\n",
    "- метод слабо чувствителен к выбору С, $\\gamma$\n",
    "- устойчив к погрешностям оценивания $\\hat{p_j}(y)$\n",
    "- не требует большого числа размеченых объектов l\n",
    "- хорошо подходит для категоризации текстов\n",
    "- в экериментах показывает высокую точность \n",
    "\n",
    "\n",
    "###  Итог\n",
    "- задача частичного обучения занимает промежуточное положение между классификацией и кластеризацией, но не сводится к ним\n",
    "- простые методы-обертки требуют многократного обучения, что вычислительно не выгодно\n",
    "- методы кластеризации легко адаптируются путем введения ограничений но как правило вычислительно трудоемки\n",
    "- методы классификации адаптируются сложнее но приводят к более эффективному частичному обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
