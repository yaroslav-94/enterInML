{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача обучения линейного классификатора\n",
    "Дано:    \n",
    "Обучающая выборка $X^l=(x_i, y_i)_{i=1} ^l$    \n",
    "$x_i$ - объекты, векторы из множества $X=\\mathcal{R}^n$   \n",
    "$y_i$ - метки классов, элементы множества $Y=\\{-1, +1\\}$   \n",
    "\n",
    "Найти:    \n",
    "Параметры $w \\in \\mathcal{R}^n, w_0 \\in \\mathcal{R}$ линейной модели классификаци    \n",
    "$a(x, w, w_0)=sign(\\langle x, w \\rangle-w_0)$    \n",
    "\n",
    "Критерий - минимизация эмпирического риска: $\\sum_{1 \\leq i \\leq l}[a(x_i, w, w_0)\\neq y_i]=\\sum_{1 \\leq i \\leq l}[M_i(w, w_0)<0] \\to min_{w, w_0}$, где $M_i(w, w_0)=(\\langle x, w \\rangle-w_0)y_i$ - отступ, $b(x)=\\langle x, w \\rangle-w_0$ - дискриминативная функция\n",
    "\n",
    "\n",
    "### Аппроксимация и регуляризация эмпирического риска\n",
    "Эмпирический риск - это кусочно-постоянная функция. Замена его оценкой сверху, непрерывной по параметрам\n",
    "$Q(w, w_0)=\\sum_{1 \\leq i \\leq l}[M_i(w, w_0)<0] \\leq \\sum_{1 \\leq i \\leq}(1-M_i()w, w_0)_+ + \\frac{1}{2C}||w||^2 \\to min_{w, w_0}$\n",
    "\n",
    "- Аппроксимация штрафует объекты за приближение к границе классов, увеличивая зазор между классами (первое слагаемое)\n",
    "- Регуляризация штрафует неустойчиве решения в случае мультиколлинеарности\n",
    "\n",
    "\n",
    "### Оптимальная разделяющая гиперплоскость\n",
    "Линейный классификатор: $a(x, w)=sign(\\langle w, x \\rangle-w_0), w,x \\in \\mathcal{R}^n, w_0 \\in \\mathcal{R}$\n",
    "\n",
    "Пусть выборка $X^l=(x_i, y_i)_{i=1} ^l$ линейно разделима: $\\exists w, w_0: M_i(w, w_0)=y_i(\\langle w, x_i \\rangle-w_0)>0, i=1,...,l$\n",
    "\n",
    "Нормировка: $min_{i=1,...,l} M_i(w, w_0)=1$\n",
    "\n",
    "Разделяющая полоса: $\\{x:-1 \\leq \\langle w, x \\rangle - w_0 \\leq 1 \\}$\n",
    "\n",
    "Ширина полосы: $\\frac{\\langle x_+-x_-, w\\rangle}{||w||}=\\frac{2}{||w||} \\to max$\n",
    "\n",
    "\n",
    "### Переход к линейно неразделимой выборке\n",
    "Постановка задачи в случае линейно разделимой выборки:\n",
    "$\\begin{cases}\n",
    "\\frac{1}{2}||w||^2 \\to min_{w, w_0} \\\\\n",
    "M_i(w, w_0)\\geq1, i=1,...,l\n",
    "\\end{cases}$\n",
    "\n",
    "Общий случай - линейно неразделимая выборка:\n",
    "$\\begin{cases}\n",
    "\\frac{1}{2}||w||^2 + C\\sum_{1 \\leq i \\leq l}\\xi_i \\to min_{w, w_0, \\xi} \\\\\n",
    "M_i(w, w_0)\\geq1-\\xi_i, i=1,...,l \\\\\n",
    "\\xi_i \\geq 0, i=1, ..., l\n",
    "\\end{cases}$\n",
    "Исключая $\\xi$ получаем задачу безусловной минимизации: $C\\sum_{1 \\leq i \\leq l}(1-M_i(w, w_0))_++\\frac{1}{2}||w||^2 \\to min_{w, w_0}$ \n",
    "\n",
    "\n",
    "### Условия Каруша-Куна-Таккера\n",
    "Задача математического программирования:\n",
    "$\\begin{cases}\n",
    "f(x) \\to min_x \\\\\n",
    "g_i(x) \\leq 0, i=1, ..., m \\\\\n",
    "h_j(x)=0, j=1, ..., k\n",
    "\\end{cases}$\n",
    "\n",
    "Необходимые условия. Если х-точка локального минимума, то существуют множители $\\mu_i, i=1,...,m; \\lambda_j, j=1,...,k$:\n",
    "$\\begin{cases}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x}=0, \\mathcal{L}(x, \\mu, \\lambda)=f(x)+\\sum_{1 \\leq i \\leq m}\\mu_ig_i(x)+\\sum_{1 \\leq j \\leq k}\\lambda_jh_j(x) \\\\\n",
    "g_i(x) \\geq 0, h_j(x)=0 \\ (исходные\\ ограничения)\\\\\n",
    "\\mu_i\\leq0\\ (двойственные\\ ограничения)\\\\\n",
    "\\mu_ig_i(x)=0\\ (условие\\ дополняющей\\ нежесткости)\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "### Применение условий ККТ к задаче SVM\n",
    "Функция Лагранжа:\n",
    "$\\mathcal{L}(w, w_0, \\xi, \\lambda, \\eta)=\\frac{1}{2}||w||^2-\\sum_{1 \\leq i \\leq l}\\lambda_i(M_i(w, w_0)-1)-\\sum_{1 \\leq i \\leq l}\\xi_i(\\lambda_i+\\eta_i-C)$\n",
    "\n",
    "$\\lambda_i$ - переменные, двойственные к ограничениям $M_i\\geq1-\\xi_i$\n",
    "\n",
    "$\\eta_i$ - переменные, двойственные к ограничениям $\\xi_i\\geq0$\n",
    "\n",
    "$\\begin{cases}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}=0, \\frac{\\partial \\mathcal{L}}{\\partial w_0}=0, \\frac{\\partial \\mathcal{L}}{\\partial \\xi}=0\\\\\n",
    "\\xi_i\\geq0, \\lambda_i\\geq0, \\eta_i\\geq0, i=1,...,l\\\\\n",
    "\\lambda_i=0\\ либо\\ M_i(w, w_0)=1-\\xi_i,i=1,...,l\\\\\n",
    "\\eta_i=0\\ либо\\ \\xi_i=0, i=1,...,l\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "### Двойственная задача\n",
    "\n",
    "$\\begin{cases}\n",
    "-\\sum_{1 \\leq i \\leq l}\\lambda_i + \\frac{1}{2}\\sum_{1 \\leq i \\leq l}\\sum_{1 \\leq j \\leq l}\\lambda_i\\lambda_j y_i y_j \\langle x_i, x_j \\rangle \\to min_{\\lambda} \\\\\n",
    "0 \\leq \\lambda_i \\leq C, i=1,...,l \\\\\n",
    "\\sum_{1 \\leq i \\leq l}\\lambda_i y_i=0\n",
    "\\end{cases}$\n",
    "\n",
    "Решив эту задачу численно относительно $\\lambda_i$, получаем линейный классификатор $a(x)=sign(\\sum_{1 \\leq i \\leq l}\\lambda_i y_i \\langle x_i, x \\rangle -w_0)$, где $w_0=\\sum_{1 \\leq i \\leq l}\\lambda_i y_i \\langle x_i, x \\rangle-y_j$ для такого j, что $\\lambda_j>0, M_j=1$\n",
    "\n",
    "Объект $x_i$ называется опорным, если $\\lambda_i \\neq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод опорных векторов. Обобщение для нелинейного случая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Двойственная задача\n",
    "\n",
    "$\\begin{cases}\n",
    "-\\sum_{1 \\leq i \\leq l}\\lambda_i + \\frac{1}{2}\\sum_{1 \\leq i \\leq l}\\sum_{1 \\leq j \\leq l}\\lambda_i\\lambda_j y_i y_j \\langle x_i, x_j \\rangle \\to min_{\\lambda} \\\\\n",
    "0 \\leq \\lambda_i \\leq C, i=1,...,l \\\\\n",
    "\\sum_{1 \\leq i \\leq l}\\lambda_i y_i=0\n",
    "\\end{cases}$\n",
    "\n",
    "Решив эту задачу численно относительно $\\lambda_i$, получаем линейный классификатор $a(x)=sign(\\sum_{1 \\leq i \\leq l}\\lambda_i y_i \\langle x_i, x \\rangle -w_0)$, где $w_0=\\sum_{1 \\leq i \\leq l}\\lambda_i y_i \\langle x_i, x \\rangle-y_j$ для такого j, что $\\lambda_j>0, M_j=1$\n",
    "\n",
    "Объект $x_i$ называется опорным, если $\\lambda_i \\neq 0$\n",
    "\n",
    "\n",
    "### Двойственная задача: нелинейное обобщение SVM\n",
    "(Меняем салярное произведение векторов на функцию $K(x_i, x_j)$)\n",
    "\n",
    "$\\begin{cases}\n",
    "-\\sum_{1 \\leq i \\leq l}\\lambda_i + \\frac{1}{2}\\sum_{1 \\leq i \\leq l}\\sum_{1 \\leq j \\leq l}\\lambda_i\\lambda_j y_i y_j K(x_i, x_j) \\to min_{\\lambda} \\\\\n",
    "0 \\leq \\lambda_i \\leq C, i=1,...,l \\\\\n",
    "\\sum_{1 \\leq i \\leq l}\\lambda_i y_i=0\n",
    "\\end{cases}$\n",
    "\n",
    "Решив эту задачу численно относительно $\\lambda_i$, получаем линейный классификатор $a(x)=sign(\\sum_{1 \\leq i \\leq l}\\lambda_i y_i K(x_i, x_j) -w_0)$, где $w_0=\\sum_{1 \\leq i \\leq l}\\lambda_i y_i K(x_i, x_j)-y_j$ для такого j, что $\\lambda_j>0, M_j=1$\n",
    "\n",
    "Объект $x_i$ называется опорным, если $\\lambda_i \\neq 0$\n",
    "\n",
    "\n",
    "### Ядра для нелинейного обобщения SVM\n",
    "Функция от пары объектов $K(x, \\acute{x})$ называется ядром, если она представмимма в виде скалярного произведения $K(x, \\acute{x})=\\langle \\varphi(x), \\varphi(\\acute{x})\\rangle$ при некотором преобразовании $\\varphi:X\\to H$ из пространства признаков Х в новое пространство Н.\n",
    "\n",
    "Признак $f_i(x)=K(x_i, x)$ - это оценка близости объекта х к опорному объекту $x_i$. Выбирая опорные объекты, SVM осуществляет отбор признаков в линейном классификаторе $a(x)=sign(\\sum_{1 \\leq i \\leq l}\\lambda_iy_iK(x_i, x)-w_0)$\n",
    "\n",
    "Примеры ядер:\n",
    " - $K(x, \\acute{x})=(\\langle x, \\acute{x} \\rangle+d)^d$ полиномиальная разделяющая поверхность \n",
    " - $K(x, \\acute{x})=\\sigma(\\langle x, \\acute{x} \\rangle)$ нейронная сеть с заданной функцией активации\n",
    " - $K(x, \\acute{x})=th(k_1\\langle x, \\acute{x} \\rangle-k_0), k_0,k_1 \\geq 0$ нейросеть с сигмоидными функциями активации\n",
    " - $K(x, \\acute{x})=exp(-\\gamma || x, \\acute{x} ||^2)$ сеть радиальных базисных функций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача обучения линейного классификатора\n",
    "Дано:    \n",
    "Обучающая выборка $X^l=(x_i, y_i)_{i=1} ^l$    \n",
    "$x_i$ - объекты, векторы из множества $X=\\mathcal{R}^n$   \n",
    "$y_i$ - метки классов, элементы множества $Y=\\{-1, +1\\}$   \n",
    "\n",
    "Линейная модель классификаци    \n",
    "$a(x, w, w_0)=sign(\\langle x, w \\rangle-w_0)$    \n",
    "\n",
    "Непрерывная аппроксимация бинарной фукнции потерь:\n",
    "$Q(w)=\\sum_{1 \\leq i \\leq l}[a(x_i, w)y_i<0] \\geq \\sum_{1 \\leq i \\leq l}\\mathcal{L}(\\langle x_i, w \\rangle y_i) \\to min_w$    \n",
    "$M_i(w, w_0)=(\\langle x_i, w \\rangle-w_0)y_i$ - отступ\n",
    "\n",
    "$\\mathcal{L}(M)=log(1+e^{-M})$ - логарифмическая функция потерь\n",
    "\n",
    "\n",
    "### Обоснование логарифмической функции потерь\n",
    "\n",
    "$(x_i, y_i)_{i=1} ^l \\backsim p(x, y, w)$ - выборка независимых наблюдений\n",
    "Принцип максимума правдоподобия: $L(w)=log\\prod p(x_i, y_i, w)=\\sum_{1 \\leq i \\leq l}log P(y_i|x_i, w)p(x_i) \\to max_w$\n",
    "\n",
    "Вероятностная модель порождения данных с параметром w\n",
    "- p(x) не зависит от параметра модели w\n",
    "- $P(y|x, w)$ описывается линейной моделью классификации\n",
    "\n",
    "$P(y_i|x_i, w)=\\frac{1}{1+exp(-\\langle x_i, w \\rangle y_i}=\\sigma(\\langle x_i, w \\rangle y_i)$, $\\sigma(M)=\\frac{1}{1+e^{-M}}$ - сигмоидная функция\n",
    "\n",
    "Тогда задачи $Q(w) \\to min$ и $L(w) \\to max$ эквивалентны: $Q(w)=\\sum_{1 \\leq i \\leq l}log(1+exp(-\\langle x_i, w \\rangle y_i)) \\to min_w$\n",
    "\n",
    "\n",
    "### Оптимизация параметров логистической регрессии\n",
    "- Метод первого порядка - стохастический градиент: $w^{(t+1)}:=w^{(t)}+\\eta_t y_i x_i (1-\\sigma_i)$\n",
    "$\\eta_t$ - градиентный шаг, $\\sigma_i=\\sigma(\\langle x_i, w \\rangle y_i)=P(y_i|x_i)$ - вероятность правильной классификации х\n",
    "- Метод второго порядка приводит к IRLS $w^{(t+1)}:=w^{(t)}+\\eta_t(F^T\\Delta F)^{-1}F^T\\tilde{y}$, F - матрица объекты-признаки, $\\tilde{y}=(y_i(1-\\sigma_i))$ - модифицированный вектор ответов, $\\Delta=diag((1-\\sigma_i)/\\sigma)$ - диагональная матрица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример применения логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кредитный скоринг\n",
    "\n",
    "Оценка риска (математического ожидания) потерь объекта: $R(x)=\\sum_{y \\in Y}D_{xy}P(y|x)=\\sum_{y \\in Y}D_{xy}\\sigma(\\langle w, x \\rangle y)$, $D_{xy}$ - величина потери для (x, y) \n",
    "\n",
    "Методика VaR (Value at Risk)\n",
    "Оценка функции распределения потерь:\n",
    " - для каждого $x_i$ разыгрывается N раз исход $y_i \\sim P(y|x_i)$\n",
    " - строится эмпирическое распределение потерь $V=\\sum_{1 \\leq i \\leq l}D_{x_i, y_i}$\n",
    " - 99% квантиль эмпирического распределения определяет величину резервируемого капитала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризованная логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_2$ регуляризация решает проблему мультиколлинеарности (сокращает веса линейно зависимых признаков): $Q(w)=\\sum_{1 \\leq i \\leq l} log(1+exp(-\\langle w, x_i\\rangle y_i))+\\tau\\sum_{1 \\leq j \\leq l}w_j ^2 \\to min_w$\n",
    "\n",
    "$L_1$ - регуляризация имеет эффект отбора признаков (обнуляет веса неинформативных признаков): $Q(w)=\\sum_{1 \\leq i \\leq l} log(1+exp(-\\langle w, x_i\\rangle y_i)) + \\tau\\sum_{1 \\leq j \\leq l} |w_j| \\to min_w$\n",
    "\n",
    "Комбинация - ElasticNet (оставляет информативные признаки при линейной зависимости)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи \n",
    "Дано:    \n",
    "Выборка $X^l=(x_1, ..., x_l)$    \n",
    "$y_i=y(x_i) \\in \\{0, 1\\}, i=1, ..., l$ - известные бинарные ответы \n",
    "$a: X \\to Y$ - алгоритм, решающая функция, приближающая y на всем множестве объектов Х\n",
    "\n",
    "### Accuracy\n",
    "Доля правильных ответов на выборке: $\\frac{1}{l}\\sum_{1 \\leq i \\leq l}[a(x_i)=y_i]$\n",
    " - соответствует интуитивным представлениям и качестве классификации\n",
    " - имеет проблемы с интерпретацией на несбалансированныхх выборках\n",
    " \n",
    "Смотреть на базовую долю правильных ответов $BaseRate=argmax_{y_0 \\in \\{0, 1\\}}\\frac{1}{l}\\sum_{1 \\leq i \\leq l}[y_0=y_i]$\n",
    "\n",
    "### Матрица ошибок\n",
    "У разных типов ошибки разная цена\n",
    "$\n",
    " \\begin{pmatrix}\n",
    " Param & y=1 & y=0 \\\\\n",
    "  a(x)=1 & True Positive(TP) & False Positive(FP)\\\\\n",
    "  a(x)=0 &  False Negative(FN) & True Negative(TN)\n",
    " \\end{pmatrix}\n",
    "$\n",
    "\n",
    "$accuracy=\\frac{TP+TN}{TP+FP+FN+TN}$\n",
    "\n",
    "\n",
    "### Точность и полнота\n",
    "Точность (precision) - насколько можно доверять классификатору: $precision=\\frac{TP}{FP+TP}$. Чем выше точность тем меньше ложных срабатываний    \n",
    "Полнота (recall) - как много объектов класса 1 находит классификатор: $recall=\\frac{TP}{TP+FN}$. Чем выше полнота тем меньше ложных пропусков\n",
    "\n",
    "\n",
    "### Усреднение точности и полноты\n",
    "Арифметическое среднее не дает выбрать более качественный алгоритм\n",
    "Гармоническое среднее - F-мера : $F=\\frac{2*precision*recall}{precision+recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи \n",
    "Дано:    \n",
    "Выборка $X^l=(x_1, ..., x_l)$    \n",
    "$y_i=y(x_i) \\in \\{0, 1\\}, i=1, ..., l$ - известные бинарные ответы \n",
    "$b: X \\to \\mathbb{R}$ - алгоритм, оценивающий принадлежность х к классу\n",
    "\n",
    "\n",
    "### Оценки принадлжности к классу\n",
    "Классификатор имеет вид: $a(x)=[b(x)>t]$, b(x) - оценка принадлежности к классу 1, t - порог классификации\n",
    "\n",
    "\n",
    "### PR-кривая\n",
    "- Отсортируем объекты по возрастанию оценки алгоритмом\n",
    "- Перебираем все пороги классификации, начиная с максимального\n",
    "- Для каждого порого считаем точность и полноту\n",
    "- На график наносим соответствующую точку\n",
    "- Соединяем точки и получаеем искомую кривую\n",
    "\n",
    "Свойства: левая точка всегда в (0, 0); правая точка в $(1, l_+/l)$ - число объектов класса 1 в выборке => чем больше площадь под кривой, тем лучше алгоритм AUC-PRC (area under precision-recall curve) - мера качества для алгоритма (точность нормируется на число положительных прогнозов и может изменится при перемене соотношения классов; максимально возможная площадь зависит от соотношения классов; подходит при сильном дисбалансе классов)\n",
    "\n",
    "\n",
    "### ROC-кривая\n",
    "reciever operating characteristics     \n",
    "По оси х откладывается False Positive Rate - доля ошибочных положительных классификаций: $FPR=\\frac{FP}{FP+TN}$   \n",
    "1-FPR - специфичность алгоритма    \n",
    "По оси y откладывается True Positive Rate - доля правильных положительных классификаций: $TPR=\\frac{TP}{TP+FN}$   \n",
    "TPR - чувствительность алгоритма   \n",
    "\n",
    "Свойства: левая точка всегда в (0,0); правая точка всегда в (1,1); если выборка идеально разделима, то существует (0,1); чем больше площадь тем лучше качество => AUC-ROC (Area under ROC-curve) - мера качества для алгоритма (метрики качества нормируются на размеры класса; интерпретация - площадь под кривой равна вероятности того, что случайно взятый объект класса 1 получит оценку выше, чем случайно взятый объект из класса 0; проблема с дисбалансом классов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи \n",
    "Дано:    \n",
    "Выборка $X^l=(x_1, ..., x_l)$    \n",
    "$y_i=y(x_i) \\in \\{0, ..., K\\}, i=1, ..., l$ - известные ответы \n",
    "Нужно найти $a: X \\to Y$ - алгоритм, решающая фукнция\n",
    "\n",
    "\n",
    "### One-vs-All\n",
    "Строим К классификаторов, отделяющих каждый класс от остальных. \n",
    "Получаем К задач бинарной классификации:\n",
    " - $X^i=x^l$ объекты для обучения\n",
    " - $y_i ^j = [y_j=i]$ ответы\n",
    " - $b_i(x) \\in \\mathbb{R}$ оценка принадлежности\n",
    " \n",
    " $a(x)=argmax_{k=1,...,K} b_k(x)$ - получившийся алгоритм (проблема с несбалансированными выборками)\n",
    " \n",
    " \n",
    "### All-vs-All\n",
    "Строим классификаторы для каждой пары классов\n",
    "Получаем К(К-1) задач бинарной классификации:\n",
    " - $X^{km}=\\{x \\in X^l | y(x)=k\\ y(x)=m \\}$ - выбираем объекты\n",
    " - $y_j ^km=[y_j=k]$ - ответы\n",
    " - $b_{km}(x) \\in \\mathbb{R}$ - оценка принадлежности\n",
    " - $b_{km}(x) = - b_{mk}(x)$ - свойство симметрии\n",
    " \n",
    " $a(x)=argmax_{k=1,...,K} \\sum_{i \\leq m \\leq K}b_{mk}(x)$ - получившийся алгоритм\n",
    " \n",
    " \n",
    "### Метрики качества\n",
    "Доля правильных ответов\n",
    "Матрица ошибок\n",
    "Микро-усреднение: найти TP, FP, FN, TN для каждой задачи; усреднение по всем задачам; вычисление итогой метрики (зависит от размера класса)\n",
    "Макро-усреднение: вычисление итоговой метрики для каждой задачи; усреднение по кдассам (классы вносят равный вклад)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsi73&$hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
