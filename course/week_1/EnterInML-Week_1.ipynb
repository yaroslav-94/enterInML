{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формальная постановка задачи машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача обучения по прецендентам\n",
    "X - множество объектов\n",
    "Y - множество ответов\n",
    "y: X $\\to$ Y - неизвестная зависимость\n",
    "\n",
    "Дано:\n",
    "$\\{x_1, ..., x_l\\}\\subset X$ - обучающая выборка, \n",
    "$y_i = y(x_i), i=1, ..., l$ - известные ответы.\n",
    "\n",
    "Найти:\n",
    "$a:X\\to Y$ -алгоритм, решающую функцию, приближающую y на всем множестве X\n",
    "\n",
    "\n",
    "### Как задаются объекты. Признаковое описание\n",
    "$f_j: X\\to D_j, j=1, ..., n$ - признаки объектов /фичи\n",
    "\n",
    "Типы признаков: \n",
    " > $D_j=\\{0, 1\\}$ - бинарный признак $f_j$;    \n",
    " > $|D_j|<\\infty $ - номинальный признак $f_j$;    \n",
    " > $|D_j|<\\infty, D_j упорядочено$ - порядковый признак $f_j$;   \n",
    " > $D_j=R$ - количественный признак $f_j$.    \n",
    "\n",
    "Вектор $(f_1(x), ..., f_n(x))$ - признаковое описание объекта x.    \n",
    "Матрица \"объекты-признаки\"\n",
    "$F=||f_j(x_i)||_{l*n}=\\begin{pmatrix}\n",
    "f_1(x_1) & \\cdots & f_n(x_n) \\\\\n",
    "\\cdots  & \\cdots  & \\cdots \\\\\n",
    "f_1(x_l) & \\cdots & f_n(x_l)\n",
    "\\end{pmatrix}$\n",
    "\n",
    "\n",
    "### Как задаются ответы. Типы задач \n",
    "\n",
    "Задачи классификации: \n",
    " > $Y=\\{-1, +1\\}$ - классификация на 2 класса;   \n",
    " > $Y=\\{1, ..., M\\}$ - на М непересекающихся классов;   \n",
    " > $Y=\\{0, 1\\}^M$ на М классов, которые могут пересекаться\n",
    " \n",
    "Задачи восстановления регрессии\n",
    " > $Y=R$ или $Y=R^m$\n",
    " \n",
    "Задачи ранжирования\n",
    " > Y - конечное упорядоченное множество\n",
    " \n",
    " \n",
    "### Пресказательная модель\n",
    "Модель - параметрическое семейство функций $A=\\{a(x)=g(x, \\theta) | \\theta \\in \\Theta\\}$,     $g: X * \\Theta \\to Y$ - фиксированная функция, $\\Theta$ - множество допустимых значений параметра $\\theta$\n",
    "\n",
    "Пример\n",
    "Линейная модель с вектором параметров $\\theta=(\\theta_1, ..., \\theta_n), \\Theta = R^n$\n",
    "\n",
    " > $g(x, \\theta)=\\sum_{\\substack{1<j<n}}\\theta_j f_j(x)$ - для регрессии и ранжирования, $Y=R$,  \n",
    " > $g(x, \\theta)=sign\\sum_{\\substack{1<j<n}}\\theta_j f_j(x)$ - для классификации, $Y=\\{-1, +1\\}$\n",
    " \n",
    "\n",
    "###  Этапы обучения и применения модели\n",
    "Этап обучения. Метод обучения $\\mu: (X * Y)^l \\to A$ по выборке $X^l=(x_i, y_i)^l _{i=1}$ строит алгоритм $a=\\mu(X^l)$: $\\begin{pmatrix}\n",
    "f_1(x_1) & \\cdots & f_n(x_n) \\\\\n",
    "\\cdots  & \\cdots  & \\cdots \\\\\n",
    "f_1(x_l) & \\cdots & f_n(x_l)\n",
    "\\end{pmatrix} \\to^y \\begin{pmatrix}y_1 \\\\ \\cdots \\\\ y_l\\end{pmatrix} \\to^\\mu a$\n",
    "\n",
    "Этап применения: алгоритм а для новых объектов $x_1 ^{\\prime}, ..., x_k ^{\\prime}$ выдает ответы $a(x_i^{\\prime})$\n",
    "$\\begin{pmatrix}\n",
    "f_1(x_1) & \\cdots & f_n(x_n) \\\\\n",
    "\\cdots  & \\cdots  & \\cdots \\\\\n",
    "f_1(x_l) & \\cdots & f_n(x_l)\n",
    "\\end{pmatrix} \\to^y \\begin{pmatrix}a(x_1^{\\prime}) \\\\ \\cdots \\\\a(x_k^{\\prime})\\end{pmatrix}$\n",
    "\n",
    "\n",
    "### Функционалы качества\n",
    "\n",
    "L(a, x) - функция потерь - величина ошибки алгоритма $a \\in A$ на объекте $x \\in X$\n",
    "\n",
    "Функции потерь для задач классификации:\n",
    " > $L(a, x)=[a(x)\\neq y(x)]$ - индикаторр ошибки\n",
    " \n",
    "Функции потерь для задач регрессии\n",
    " > $L(a, x)=|a(x)-y(x)|$ - абсолютное значение ошибки;    \n",
    " > $L(a, x)=(a(x)-y(x))^2$ - квадратичная ошибка\n",
    " \n",
    "Эмпирический риск - функционл качества алгоритма a на $X^l$:\n",
    "\n",
    "$Q(a, X^l)=\\frac{1}{l}\\sum_{\\substack{1<i<l}}L(a, x_i)$\n",
    "\n",
    "\n",
    "### Сведение задачи обучения к задаче оптимизации\n",
    "\n",
    "Минимизация эмпирического риска: $\\mu(X^l)=arg min_{a \\in A}Q(a, X^l)$\n",
    "\n",
    "Метод наименьших квадратов: $\\mu(X^l)=arg min_\\theta \\sum_{\\substack{1\\leq i \\leq l}}(g(x_i, \\theta)-y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема переобучения. Методология решения задач машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эмпирические оценки обобщающей способности\n",
    "\n",
    "> Эмпирический риск на тестовых данных (hold-out) $HO(\\mu, X^l, X^k)=Q(\\mu(X^l), X^k) \\to min $\n",
    "\n",
    "> Скользящий контроль (leave-one-out), L=l+1: $LOO(\\mu, X^l)=\\frac{1}{L}\\sum_{\\substack{1<i<L}}\\mathcal{L}(\\mu(X^l|{x_i}), x_i) \\to min$\n",
    "\n",
    "> Кросс-проверка по N разбиениям: $X^L=X^l _n \\sqcup X^k _n, L=l+k$, $CV(\\mu, X^L)=\\frac{1}{N}\\sum_{\\substack{1<n<N}}Q(\\mu(X^l _n), X^l _n) \\to min$\n",
    "\n",
    "CRISP-DM - межотраслевой стандарт решения задач интеллектуального анализа данных\n",
    "\n",
    "### Этапы решения задач машинного обучения:\n",
    " - понимание задачи и данных\n",
    " - предобработка данных и изобретение признаков\n",
    " - построение модели\n",
    " - сведение обучения к оптимизации\n",
    " - решение проблем оптимизации и переобучение\n",
    " - оценивание качества решения\n",
    " - внедрение и эксплуатация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с векторами и матрицами в NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 27.62959775  -0.03044889  -2.37893263 ...   0.7907481    0.84448831\n",
      "   13.12526703]\n",
      " [ 14.27896603   9.89167151   4.86712915 ...  10.07484319   7.72718931\n",
      "   -0.57056471]\n",
      " [-14.36143297   1.54551245  10.79341614 ...   8.19946187   1.27141173\n",
      "    6.05333731]\n",
      " ...\n",
      " [  6.2984016    3.19884844 -17.50667425 ...   6.93424378  -5.44290163\n",
      "    4.36831041]\n",
      " [ -2.11911511  14.2266969  -14.53945215 ...  -6.87622081  13.06748917\n",
      "    6.42436751]\n",
      " [ -7.49068918  -5.16304102   4.60864366 ...   6.73378756   0.54972655\n",
      "   16.50783537]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Сгенерируйте матрицу, состоящую из 1000 строк и 50 столбцов, элементы которой являются случайными из нормального распределения N(1,100).\n",
    "\n",
    "X = np.random.normal(loc=1, scale=10, size=(1000, 50))\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.61379095e+00 -8.15621664e-02 -3.24853376e-01 ... -6.03544090e-02\n",
      "   1.67171782e-03  1.20620086e+00]\n",
      " [ 1.28084904e+00  9.30913104e-01  3.98854578e-01 ...  8.81140424e-01\n",
      "   6.93647776e-01 -1.54516560e-01]\n",
      " [-1.57864024e+00  7.92524401e-02  9.90748653e-01 ...  6.90959065e-01\n",
      "   4.45939348e-02  5.03585777e-01]\n",
      " ...\n",
      " [ 4.84060615e-01  2.47962527e-01 -1.83575230e+00 ...  5.62654014e-01\n",
      "  -6.30452691e-01  3.36173860e-01]\n",
      " [-3.56353613e-01  1.37326875e+00 -1.53939791e+00 ... -8.37857391e-01\n",
      "   1.23055322e+00  5.40448617e-01]\n",
      " [-8.92657540e-01 -6.05303302e-01  3.73038071e-01 ...  5.42325862e-01\n",
      "  -2.79631705e-02  1.54226805e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Произведите нормировку матрицы из предыдущего задания: вычтите из каждого столбца его среднее значение, а затем поделите на его стандартное отклонение.\n",
    "\n",
    "m = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X_norm = ((X - m)  / std)\n",
    "print (X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 4, 5]),)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Выведите для заданной матрицы номера строк, сумма элементов в которых превосходит 10.\n",
    "\n",
    "Z = np.array([[4, 5, 0], \n",
    "             [1, 9, 3],              \n",
    "             [5, 1, 1],\n",
    "             [3, 3, 3], \n",
    "             [9, 9, 9], \n",
    "             [4, 7, 1]])\n",
    "\n",
    "r = np.sum(Z, axis=1)\n",
    "print (np.nonzero(r > 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Сгенерируйте две единичные матрицы (т.е. с единицами на диагонали) размера 3x3. Соедините две матрицы в одну размера 6x3.\n",
    "\n",
    "A = np.eye(3)\n",
    "B = np.eye(3)\n",
    "AB = np.vstack((A, B))\n",
    "print (AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решающие деревья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача классификации (обучение с учителем)\n",
    "\n",
    "Задача восстановления зависимсти $y: X \\to Y, |Y|<\\infty$ по точкам обучающей выборки $(x_i, y_i), i=1,...,l$\n",
    "\n",
    "Дано: \n",
    "    векторы $x_i=(x_i ^1, ..., x_i ^n)$ - объекты обучающей выборки, $y_i=y(x_i)$ - классификации, ответы учителя, i=1, ..., l:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "(x_1 ^1) & \\cdots & x_1 ^n \\\\\n",
    "\\cdots  & \\cdots  & \\cdots \\\\\n",
    "(x_1 ^l) & \\cdots & x_l ^n \n",
    "\\end{pmatrix} \\to^y \\begin{pmatrix}y_1 \\\\ \\cdots \\\\ y_l\\end{pmatrix}$\n",
    "\n",
    "Найти: \n",
    "    функцию a(x), способную классифицировать объекты произвольной тестовой выборки $\\tilde{x_i}=(\\tilde{x_i ^1}, ..., \\tilde{x_i ^n}), i=1, ..., k$:\n",
    "$\\begin{pmatrix}\n",
    "\\tilde{x_i ^1} & \\cdots & \\tilde{x_i ^n} \\\\\n",
    "\\cdots  & \\cdots  & \\cdots \\\\\n",
    "\\tilde{x_k ^1} & \\cdots & \\tilde{x_k ^n}\n",
    "\\end{pmatrix} \\to^{a?} \\begin{pmatrix}a(\\tilde{x_1}) \\\\ \\cdots \\\\ \\tilde{x_k}\\end{pmatrix}$\n",
    "\n",
    "### Определение бинарного решающего дерева\n",
    "Бинарное решающее дерево - алгоритм классификации a(x), задающийся бинарным деревом: \n",
    " > 1) $\\forall v \\in V_{внутр} \\to$ предикат $\\beta_v: X\\to \\{0,1\\}, \\beta_v \\in \\mathcal{B}$, \n",
    " \n",
    " > 2) $\\forall v \\in V_{внутр} \\to$ имя класса $c_v \\in Y$, где $\\mathcal{B}$ - множество бинарных признаков или предикатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм построения решающего дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Жадный алгоритм построения дерева ID3\n",
    "\n",
    "1) Процедура LearnID3 $(U \\subseteq X^l)$   \n",
    "2) если все объекты из U лежат в одном классе $c \\in Y$, то    \n",
    "3) вернуть новый лист v, $c_v:=c$  \n",
    "4) найти предикат с максимальной онформативностью: $\\beta:=arg max_{\\beta \\in \\mathcal{B}} I (\\beta, U)$  \n",
    "5) разбить выборку на две части $U=U_0\\sqcup U_1$ по предикату $\\beta$:\n",
    "$U_0:=\\{x\\in U: \\beta(x)=0\\}$, $U_1:=\\{x\\in U: \\beta(x)=1\\}$  \n",
    "6) если $U_0=\\oslash$ или $U_1=\\oslash$ то  \n",
    "7) вернуть новый лист v, $c_v$:=Мажоритарный класс(U)  \n",
    "8) создать новую внутреннюю вершину $v:\\beta_v:=\\beta$ построить левое поддерево: $L_v:=LearnID3(U_0)$, построить правое поддерево   $R_v:=LearnID3(U_1)$   \n",
    "9) вернуть v\n",
    "\n",
    "\n",
    "### Варианты критериев ветвления\n",
    "1) Критерий Джини $I(\\beta, X^l)=\\#\\{(x_i, x_j):y_i=y_j, \\beta(x_i)=\\beta(x_j)\\}$    \n",
    "2) D-критерий Донского: $I(\\beta, X^l)=\\#\\{(x_i, x_j):y_i\\neq y_j, \\beta(x_i)\\neq \\beta(x_j)\\}$\n",
    "3) Энтропийный критерий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка пропусков. Достоинства и недостатки решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка пропусков\n",
    "\n",
    "На стадии обучения:\n",
    " > $\\beta_v(x)$ - не определено, $x_i$ исключается из U для $I(\\beta, U)$\n",
    " \n",
    " > $q_v=\\frac{|U_0|}{|U|}$ - оценка вероятности левой ветви, $\\forall v \\in V_{внутр}$ \n",
    " \n",
    " > функция условной вероятности $P(y|x, v)=\\frac{1}{|U|}\\#{x_i \\in U, y_i=y}$ для всех $v \\in V_{лист}$\n",
    " \n",
    "На стадии классификации:\n",
    " > $\\beta_v(x)$ не определено => пропорциональное распределение:\n",
    " $P(y|x, v)=q_vP(y|x, L_v)+(1-q_v)P(y|x, R_v)$\n",
    " \n",
    " > $\\beta_v(x)$ определено => либо налево, либо направо:\n",
    " $P(y|x, v)=(1-\\beta_v(x))P(y|x, L_v)+\\beta_v(x)P(y|x, R_v)$\n",
    " \n",
    " > окончательное решение - наиболее вероятный класс: \n",
    " $a(x)=argmax_{y \\in Y}P(y|x, v_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способы устранения недостатков решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Усечение дерева (pruning). Алгоритм С4.5\n",
    "Если проблема с рещающим деревом заключается в переобучении, переусложнении структуры дерева, значит, нужна контрольная выборка данных, для того чтобы оценивать обобщающую способность \n",
    "\n",
    "1) для всех $v \\in V_внутр$     \n",
    "2) $S_v:=$ подмножество объектов $X^k$, дошедших до v    \n",
    "3) если $S_v=\\oslash$ то    \n",
    "4) вернуть новый лист v, $c_v:=$ мажоритрный класс(U)    \n",
    "5) число ошибок при классификации $S_v$ четырмя способами    \n",
    "   - r(v) - поддеревом, растущим из вершины v;\n",
    "   - $r_L(v)$ - поддеревом левой дочерней вершины $L_v$;\n",
    "   - $r_R(v)$ - поддеревом правлй дочерней вершины $R_v$;\n",
    "   - $r_c(v)$ - к классу $c \\in Y$\n",
    "6) в зависимости от того, какое из них минимально:\n",
    "   - сохранить поддерево v;\n",
    "   - заменить поддерево v поддеревом $L_v$;\n",
    "   - заменить поддерево v поддеревом $R_v$;\n",
    "   - заменить поддерево v листом $c_v:=arg min_{c \\in Y}r_c(v)$\n",
    "   \n",
    "   \n",
    "### CART: деревья регрессии и классификации\n",
    "Обобщение на случай регрессии: $Y=\\mathbb{R}, c_v \\in \\mathbb{R}$\n",
    "Пусть $U_v$ - множество объектов $x_i$, дошедших до вершины v. Значения в терминальных вершинах - МНК-решение: $c_v:=\\hat{y}(U_v)=\\frac{1}{|U_v|} \\sum_{\\substack{x_i \\in U_v}}y_i$\n",
    "\n",
    "Критерий информативности - среднеквадратичная ошибка $I(\\beta, U_v)=\\sum_{\\substack{x_i \\in U_v}}(\\hat{y_i}(\\beta)-y_i)^2$, где $\\hat{y_i}(\\beta)=\\beta(x_i)\\hat{y_i}(U_{v1}) + (1-\\beta(x_i))\\hat{y_i}(U_{v0})$ - прогноз ветвления $\\beta$ и разбиения $U_v=U_{v0}\\sqcup U_{v1}$\n",
    "\n",
    "Итог: кусочно-постоянная функция\n",
    "\n",
    "\n",
    "### CART: критерий Minimal Cost-Complexity Pruning\n",
    "Среднекввадратичная ошибка со штрафом за сложность дерева \n",
    "$C_{\\alpha}=\\sum_{1\\leq x_i \\leq l}(\\hat{y_i}-y_i)^2 + \\alpha|V_{лист}|\\to min$\n",
    "\n",
    "При увеличении $\\alpha$ дерево последовательно упрощается. Причем последовательность вложенных деревьев единственна. Из этой последовательности выбирается дерево с минимальной ошибкой на тестовой выборке. Для случая классификации используется аналогичная стратегия усчения, с критерием Джини."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
