{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гипотезы компактности и непрерывности\n",
    "\n",
    "Гипотеза непрерывности (регрессия) - близким объектам соответствуют близкие ответы\n",
    "\n",
    "Гипотеза компактности (классификация) - близкие объекты лежат в одном классе\n",
    "\n",
    "Формализация понятия расстояния: задана функция расстояния $\\rho:X*X \\to [0, \\infty)$\n",
    "\n",
    "\n",
    "### Обобщенный метрический классификатор\n",
    "Для произвольного $x \\in X$ отранжируем объекты $x_1, ..., x_l$:   \n",
    "$\\rho(x, x^{(1)})\\leq \\rho(x, x^{(2)})\\leq ... \\leq \\rho(x, x^{(l)})$\n",
    "\n",
    "$x^{(i)}$ - i-й сосед объекта х среди $x_1, ..., x_l$\n",
    "$y^{(i)}$ - ответ на i-м соседе объекта х\n",
    "\n",
    "Метрический алгоритм классификации:   \n",
    "$a(x, X^l)=argmax_{y \\in Y} \\sum_{1\\leq i \\leq l}[y^{(i)}=y]w(i, x)$\n",
    "\n",
    "w(i, x) - вес, оценка сходства объекта х с его i-м соседом, неотрицательная, не возрастающая по i    \n",
    "$\\Gamma_y(x)=\\sum_{1\\leq i \\leq l}[y^{(i)}=y]w(i, x)$ - оценка близости объекта х к классу y\n",
    "\n",
    "\n",
    "### Метод ближайших соседей kNN\n",
    "w(i, x) = [i=<k]\n",
    "\n",
    "Параметр k можно оптимизировать по критерию скользящего контроля (leave-one-out): $LOO(k, X^l)=\\sum_{1 \\leq i \\leq l}[a(x_i, X^l|\\{x_i\\}, k)\\neq y_i] \\to min_q$\n",
    "\n",
    "Неоднозначность классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод окна Парзена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ищем окрестность с фиксированной точкой отсчета\n",
    "\n",
    "$w(i, x)=K(\\frac{\\rho(x, x^{(j)})}{h})$, h - ширина окна    \n",
    "K(r) - ядро, не возрастает и положительно на [0, 1]\n",
    "\n",
    "Метод парзеновского окна фиксированной ширины:\n",
    "$a(x, X^l, h, K)=argmax_{y \\in Y}\\sum_{1 \\leq i \\leq l}[y_i=y]K(\\frac{\\rho(x, x^{(j)})}{h})$\n",
    "\n",
    "Метод парзеновского окна переменной ширины:\n",
    "$a(x, X^l, h, K)=argmax_{y \\in Y}\\sum_{1 \\leq i \\leq l}[y_i=y]K(\\frac{\\rho(x, x_i)}{{\\rho(x, x^{(k+1)})}})$\n",
    "\n",
    "Оптимизация например по LOO\n",
    "Ядра не сильно вляют на качество\n",
    "\n",
    "\n",
    "### Метод потенциальных функций\n",
    "$w(i, x)=\\gamma^{(i)}K(\\frac{\\rho(x, x^{(i)})}{h^{(i)}})$\n",
    "\n",
    "$a(x, X^l)=argmax_{y \\in Y}\\sum_{1\\leq i \\leq l}[y_i=y]\\gamma_i K(\\frac{\\rho(x, x_i)}{h_i})$\n",
    "\n",
    "\n",
    "### Метод потенциальных функций - линейный классификатор\n",
    "Два класса: Y={-1, +1}\n",
    "$a(x, X^l)=argmax_{y \\in Y} \\Gamma_y (x)=sign(\\Gamma_{+1} (x)-\\Gamma_{-1} (x))=sign \\sum_{1 \\leq i \\leq l} \\gamma_i y_i K(\\frac{\\rho(x, x_i)}{h_i})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрические методы классификации в задаче восстановления регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Задачи регрессии и метод наименьших квадратов\n",
    "\n",
    "Х - объекты ($\\mathbb{R}^n$), Y - ответы ($\\mathbb{R}, \\mathbb{R}^m$)   \n",
    "$X^l=(x_i, y_i)_{i=1}^l$ - обучающая выборка   \n",
    "$y_i=y(x_i), y:X \\to Y$ - неизвестна зависимость\n",
    "$a(x)=f(x, \\alpha)$ - параметрическая модель зависимости, $\\alpha \\in \\mathbb{R}^p$ - вектор параметров модели   \n",
    "Метд наименьших квадратов (МНК): $Q(\\alpha, X^l)=\\sum_{1 \\leq i \\leq l}w_i(f(x_i, \\alpha)y_i)^2 \\to min_{\\alpha}$, w-вес\n",
    "\n",
    "$f(x, \\alpha)$ - нужно знать заранее и хорошо ее настроить\n",
    "\n",
    "\n",
    "### Непараметрическая регрессия. Формула Надарая-Ватсона\n",
    "\n",
    "Приближение константой $f(x, \\alpha)=\\alpha$ в окрестности $x \\in X$\n",
    "\n",
    "$Q(\\alpha, X^l)=\\sum_{1\\leq i \\leq l}w_i(x)(\\alpha - y_i)^2\\to min_{\\alpha \\in \\mathbb{R}}$\n",
    "\n",
    "$w_i(x)=K(\\frac{\\rho(x, x_i)}{h})$ - веса объектов $x_i$ от носительно х, K(r) - ядро, невозрастающее, ограниченное, гладкое [0, 1], h - ширина окна сглаживания\n",
    "\n",
    "Формула ядерного сглаживания Надарая-Ватсона\n",
    "$\\alpha_h(x, X^l)=\\frac{\\sum_{1 \\leq i \\leq l}y_iw_i(x)}{\\sum_{1 \\leq i \\leq l}w_i(x)}=\\frac{\\sum_{1 \\leq i \\leq l}y_iK(\\frac{\\rho(x, x_i)}{h})}{\\sum_{1 \\leq i \\leq l}K(\\frac{\\rho(x, x_i)}{h})}$\n",
    "\n",
    "Ширина окна существенно влияет на качество аппроксимации. Нужно использовать оптимизацию по скользящему контролю, либо использовать переменную ширину окна в зависимости от концентрации объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обнаружение выбросов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема выбросов и локально взвешенное сглаживание\n",
    "\n",
    "большие случайные ошибки в значениях $y_i$ сильно искажает оценку Надарая-Ватсона\n",
    "\n",
    "$a_h(x, X^l)=\\frac{\\sum_{1\\leq i \\leq l}y_iw_i(x)}{\\sum_{1\\leq i \\leq l}w_i(x)}, w_i(x)=K(\\frac{\\rho(x, x_i)}{h})$ \n",
    "\n",
    "Чем больше величина невязки $\\epsilon_i=|a_h(x_i; X^l\\not{\\{x_i\\}})-y_i|$ тем меньше должен быть вес этого выброса в w\n",
    "\n",
    "Домноженние весов $w_i(x)$ на коэффициенты $\\gamma_i=\\tilde{K},(\\epsilon_i)$  $\\tilde{K}(\\epsilon_i)$ - еще одно ядро, отличное от K(r)\n",
    "\n",
    "использовать квартическое ядро $\\tilde{K}(\\epsilon)=K_Q(\\frac{\\epsilon}{6med(\\epsilon}))$, $med(\\epsilon_i)$ - медиана множества значений $\\epsilon_i$\n",
    "\n",
    "\n",
    "### Алгоритм LOWESS (Locally weighted scatter plot smoothing)\n",
    "\n",
    "Вход: $X^l$ - обучающая выборка\n",
    "Выход: коэффициенты $\\gamma_i, i=1, ..., l$   \n",
    "1) инициализация $\\gamma_i:=1, i=1,...,l$  \n",
    "2) повторять  \n",
    "3) для всех объектов $i=1, ..., l$  \n",
    "4) вычислить оценки скользящего контроля  \n",
    "$a_i:=a_h(x_i; X^l\\not{\\{x_i\\}})=\\frac{\\sum_{1\\leq j \\leq l, j\\neq i}y_j\\gamma_jK(\\frac{\\rho(x, x_j)}{h(x_i)})}{\\sum_{1\\leq j \\leq l, j\\neq i}\\gamma_jK(\\frac{\\rho(x, x_j)}{h(x_i)})}$   \n",
    "5) для всех объектов $i=1, ..., l$  \n",
    "6) $\\gamma_i:=\\tilde{K}(|a_i-y_i|)$   \n",
    "7) пока коэффициенты $\\gamma_i$ не стабилизируется\n",
    "\n",
    "Методы устойчивые к нарушениям модельных предположений о данных - робастные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод стохастического градиента. Постановка задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи - регрессия\n",
    "\n",
    "Обучающая выборка: $X^l=(x_i, y_i)_{i=1} ^l, x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}$\n",
    "\n",
    "Модель регресии - линейная: $a(x, w)=\\langle x, w \\rangle=\\sum_{1 \\leq j \\leq n}f_j(x)w_j, w \\in \\mathbb{R}^n$\n",
    "\n",
    "Функция потерь - квадратичная: $\\mathscr{L}(a, y)=(a-y)^2$\n",
    "\n",
    "Метод обучения - метод наименьших квадратов: $Q(w)=\\sum_{1 \\leq i \\leq l}(a(x_i, w)-y_i)^2 \\to min_w$\n",
    "\n",
    "Проверка по тестовой выборке $X^k=(\\tilde{x}_i, \\tilde{y}_i)_{i=1} ^k$: $\\tilde{Q}(w)=\\frac{1}{k}\\sum_{1 \\leq i \\leq k}(a(\\tilde{x}_i, w)-\\tilde{y}_i)^2$\n",
    "\n",
    "\n",
    "### Постановка задачи - классификация\n",
    "\n",
    "Обучающая выборка: $X^l=(x_i, y_i)_{i=1} ^l, x_i \\in \\mathbb{R}^n, y_i \\in \\{-1, +1\\}$\n",
    "\n",
    "Модель классификации - линейная: $a(x, w)=sign\\langle x, w \\rangle$\n",
    "\n",
    "Непрерывная аппроксимация бинарной фукнции потерь: $\\mathscr{L}(a, y)=[\\langle x_i, w \\rangle y_i<0] \\leq \\mathscr{L}(\\langle x_i, w \\rangle y_i)$, $M_i(w)=\\langle x_i, w \\rangle y_i$ - отступ (margin) \n",
    "\n",
    "Метод обучения - минимизация эмпирического риска: $Q(w)=\\sum_{1 \\leq i \\leq l}[\\langle x, w \\rangle y_i<0] \\leq \\sum_{1 \\leq i \\leq l} \\mathscr{L}(\\langle x, w \\rangle y_i) \\to min_w$\n",
    "\n",
    "Проверка по тестовой выборке: $X^k=(\\tilde{x}_i, \\tilde{y}_i)_{i=1} ^k$: $\\tilde{Q}(w)=\\frac{1}{k}\\sum_{1 \\leq i \\leq k} [\\langle \\tilde x_i, w \\rangle \\tilde y_i<0]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентные методы численной минимизации и алгоритм SG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный метод численной минимизации\n",
    "\n",
    "Минимизация эмпирического риска: $Q(w)=\\sum_{1 \\leq i \\leq l}[\\langle x, w \\rangle y_i] = \\sum_{1 \\leq i \\leq l} \\mathscr{L}_i (w) \\to min_w$\n",
    "\n",
    "Численная минимизация методом градиентного спуска:\n",
    "\n",
    "$w^{(0)}:=$ начальное приближение\n",
    "\n",
    "$w^{(t+1)}:=w^{(t)}-h\\nabla Q(w^{(t)})$ $\\nabla Q(w)=(\\frac{\\partial Q(w)}{\\partial w_j})_{j=0} ^n$\n",
    "\n",
    "h - градиентный шаг (темп обучения)\n",
    "\n",
    "$w^{(t+1)}:=w{(t)}-h\\sum_{1 \\leq i \\leq l} \\nabla \\mathscr{L}_i(w^{(t)})$\n",
    "\n",
    "Идея ускорения сходимости: брать $(x_i, y_i)$ по одному и сразу обновлять вектор весов\n",
    "\n",
    "\n",
    "### Алгоритм SG (Stochastic Gradient)\n",
    "Вход: выборка $X^l$, темп обучения h, темп забывания $\\lambda$   \n",
    "Выход: вектор весов w    \n",
    "\n",
    "1) инициализировать веса $w_j, j=0,...,n$    \n",
    "2) инициализировать оценку функционала $\\bar{Q}:=\\frac{1}{l}\\sum_{1 \\leq i \\leq l}\\mathscr{L}_i(w)$   \n",
    "3) повторять    \n",
    "4) выбрать объект $x_i$ из $X^l$ случайным образом     \n",
    "5) вычислить потерю $\\epsilon_i:=\\mathscr{L}_i(w)$   \n",
    "6) сделать градиентный шаг $w:=w-h\\nabla\\mathscr{L}_i(w)$    \n",
    "7) оценить функционал $\\bar{Q}:=(1-\\lambda)\\bar{Q}+\\lambda\\epsilon_i$  \n",
    "8) пока значение $\\bar{Q}$ и/или веса w не сойдутся\n",
    "\n",
    "\n",
    "### Оценка функционала\n",
    "\n",
    "Не хочется оценивать Q по всей выборке после каждого шага w по одном объекту $x_i$\n",
    "\n",
    "Использовать рекуррентную формулу.\n",
    "\n",
    "Среднее арифметическое $\\bar{Q}_m=\\frac{1}{m}\\sum_{1 \\leq i \\leq m} \\epsilon_i$: $\\bar{Q}_m=(1-\\frac{1}{m})\\bar{Q}_{m-1}+\\frac{1}{m}\\epsilon_m$\n",
    "\n",
    "\n",
    "Экспоненциальное скользящее среднее $\\bar{Q}:=(1-\\lambda)\\bar{Q}_{m-1}+\\lambda\\epsilon_m$: $\\bar{Q}_m=\\lambda\\epsilon_m+\\lambda(1)-\\lambda\\epsilon_{m-1}+\\lambda(1-\\lambda)^2\\epsilon_{m-2}+\\lambda(1-\\lambda)^3\\epsilon_{m-3}+...$\n",
    "\n",
    "\n",
    "Параметр - $\\lambda \\approx \\frac{1}{m}$ темп забывания. Чем больше $\\lambda$, тем быстрее забывается предыстория рядя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм SAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вход: выборка $X^l$, темп обучения h, темп забывания $\\lambda$   \n",
    "Выход: вектор весов w    \n",
    "\n",
    "1) инициализировать веса $w_j, j=0,...,n$  \n",
    "2) инициализировать градиенты: $G_i:=\\nabla\\mathscr{L}_i(w), i=1,...,l$   \n",
    "3) инициализировать оценку функционала $\\bar{Q}:=\\frac{1}{l}\\sum_{1 \\leq i \\leq l}\\mathscr{L}_i(w)$   \n",
    "4) повторять    \n",
    "5) выбрать объект $x_i$ из $X^l$ случайным образом     \n",
    "6) вычислить потерю $\\epsilon_i:=\\mathscr{L}_i(w)$   \n",
    "7) вычислить градиент $G_i:=\\nabla\\mathscr{L}_i(w)$   \n",
    "8) сделать градиентный шаг $w:=w-h\\sum_{1 \\leq i \\leq l}G_i$    \n",
    "9) оценить функционал $\\bar{Q}:=(1-\\lambda)\\bar{Q}+\\lambda\\epsilon_i$  \n",
    "10) пока значение $\\bar{Q}$ и/или веса w не сойдутся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод стохастического градиента. Достоинства и недостатки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достоинства:\n",
    " - легко реализуется\n",
    " - применим к любым моделям и функциям потерь\n",
    " - допускает потокове обучение\n",
    " - на сверхбольших выборках позволяет получать неплохие решения, даже без обработки всех данных\n",
    " - все чаще применяется в Big Data\n",
    " \n",
    "Недостатки:\n",
    " - возможное застревание в локальных экстремумах\n",
    " - возможная расходимость или медленная сходимость\n",
    " - возможное переобучение\n",
    " \n",
    " \n",
    "### Варианты инициализации весов\n",
    " - $w_j:=0$ для всех j=1,...,n  \n",
    " - небольшие случайные значения $w_j:=random(-\\frac{1}{2n}, \\frac{1}{2n})$ \n",
    " - $w_j:=\\frac{\\langle y, f_j \\rangle}{\\langle f_j, f_j \\rangle}, f_j=(f_j(x_i))_{i=1} ^l$ - вектор значения признака; эта оценка w оптимальна при квадратичной фукнции потерь, если признаки некоррелированы $\\langle f_j, f_k \\rangle=0, j\\neq k$   \n",
    " - $w_j:=ln(\\frac{\\sum_i[y_i=+1]f_j(x_i)\\sum_i[y_i=-1]}{\\sum_i[y_i=11]f_j(x_i)\\sum_i[y_i=+1]})$ - эта оценка w оптимальна для задач классификации, Y={-1, +1}, если признаки независимы\n",
    " - оценки $w_j$ по небольшой случайной подвыборке объектов    \n",
    " - мультистарт: многократные запуски из разных случайных начальных приближений и выбор лучшего решения\n",
    " \n",
    " \n",
    "### Варианты порядка предъвления объектов\n",
    " - перемешивние объектов\n",
    " - чаще брать те объекты, на которых была допущена большая ошибка (большой отступ)\n",
    " - вообще не брать хорошие объекты, тк они хорошо разделились (большой отступ)\n",
    " - вообще не брать плохие объекты, тк это скорее всего выбросы (большой отступ)\n",
    "Придется подбирать параметры для оценки величины отступа\n",
    "\n",
    "\n",
    "### Варианты выбора градиентного шага\n",
    "- сходимость гарантируется для выпуклых функций при $h_t \\to , \\sum_{1 \\leq t \\leq \\infty}h_t=\\infty, \\sum_{1 \\leq t \\leq \\infty}h^2 _t<\\infty$, в частности можно положить $h_t=\\frac{1}{t}$   \n",
    "- метод скорейшего градиентного спуска: $\\mathscr{L}(w-h\\nabla\\mathscr{L}_i(w)) \\to min_h$, позволят найти адаптивный шаг $h^*$ при квадратичной функции потерь $h^*=||x_i||^{-2}$\n",
    "- периодически нужно делать пробные случайные шаги для выхода из локальных минимумов\n",
    "- метод Левенберга-Марквардта второго порядка\n",
    "\n",
    "\n",
    "### Диагональный метод Левенберга-Марквардта\n",
    "Метод Ньютона-Рафсона, $\\mathscr{L}_i(w)\\equiv \\mathscr{L}(\\langle w, x_i\\rangle y_i)$, $w:=w-h(\\mathscr{L}\"_i(w))^{-1}\\nabla \\mathscr{L}_i(w)$, где $\\mathscr{L}\"_i(w)=(\\frac{\\partial^2 \\mathscr{L}(w)}{\\partial w_j \\partial w_{j'}})$ гессиан, $n*n$ матрица\n",
    "\n",
    "Эвристика: считаем, что гессиан диагонален. Тогда    \n",
    "$w_j:=w_j-h(\\frac{\\partial^2 \\mathscr{L}_i(w)}{\\partial w_j ^2}+ \\mu)\\frac{\\partial \\mathscr{L}_i (w)}{\\partial w_j}$    \n",
    "h - темп обучения, полагаем =1   \n",
    "$\\mu$ - параметр, предотвращающий обнуление знаменателя    \n",
    "Отношение $\\frac{h}{\\mu}$ есть темп обучения на ровных участках функционала    $\\mathscr{L}_i(w)$, где вторая производная обнуляется   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема переобучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Причины переобучения:\n",
    " - присутствуют коррелирующие признаки \n",
    " - мало объектов, много признаков\n",
    "\n",
    "Признаки переобучения:\n",
    " - слишком большие веса разных знаков\n",
    " - неустойчивость классификации относительно погрешностей измерения признаков\n",
    " - $Q(X^l)<<Q(X^k)$\n",
    " \n",
    "### Регуляризация (сокращение весов)\n",
    "Штраф за увеличение нормы вектора весов: $\\mathscr{\\bar L}_i(w)=\\mathscr{L}_i(w)+\\frac{\\tau}{2}||w||^2=\\mathscr{L}_i(w)+\\frac{\\tau}{2}sum_{1 \\leq j \\leq n}w_j ^2 \\to min_w$\n",
    "\n",
    "Градиент: $\\nabla\\mathscr{\\bar L}_i(w)=\\nabla\\mathscr{L}_i(w)+\\tau w$\n",
    "\n",
    "Модификация градиентного шага: $w:=w(1-\\tau h)-h\\nabla\\mathscr{L}_i(w)$\n",
    "\n",
    "Подбор параметра $\\tau$ - по скользящему контролю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
